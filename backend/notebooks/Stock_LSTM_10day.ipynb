{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20941032",
   "metadata": {},
   "source": [
    "# Stock LSTM — Unified Backtest + 10‑day Forecast\n",
    "\n",
    "This notebook is parameterized for Papermill.\n",
    "\n",
    "**Inputs:** `TICKER, LOOKBACK, CONTEXT, BACKTEST_HORIZON, HORIZON, OUTPUT_JSON`\n",
    "\n",
    "**Output file format (JSON)**:\n",
    "```json\n",
    "{\n",
    "  \"ticker\": \"AAPL\",\n",
    "  \"look_back\": 60,\n",
    "  \"context\": 100,\n",
    "  \"backtest_horizon\": 20,\n",
    "  \"horizon\": 10,\n",
    "  \"metrics\": {\n",
    "    \"rmse\": 0.0,\n",
    "    \"mape\": 0.0,\n",
    "    \"accuracy_pct\": 0.0,\n",
    "    \"expected_10d_move_pct\": 0.0\n",
    "  },\n",
    "  \"forecast\": [\n",
    "    {\"date\": \"YYYY-MM-DD\", \"actual\": 123.45, \"part\": \"context\"},\n",
    "    {\"date\": \"YYYY-MM-DD\", \"pred\": 123.45, \"part\": \"backtest\"},\n",
    "    {\"date\": \"YYYY-MM-DD\", \"pred\": 123.45, \"part\": \"forecast\"}\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cddf3f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "#  imports\n",
    "import os, math, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import tensorflow as tf\n",
    "from joblib import dump, load\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88c31424",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill parameters (will be overridden when executed by Papermill)\n",
    "TICKER = \"AAPL\"\n",
    "LOOKBACK = 60\n",
    "CONTEXT = 100\n",
    "BACKTEST_HORIZON = 20\n",
    "HORIZON = 10\n",
    "OUTPUT_JSON = \"forecast.json\"\n",
    "INTERVAL = \"1d\"\n",
    "\n",
    "# Where to read a big training universe (one symbol per line)\n",
    "TICKERS_FILE = \"tickers_1000_plus.txt\"\n",
    "\n",
    "# Training / loading behavior\n",
    "FORCE_RETRAIN_GLOBAL   = False   # True => retrain global model even if saved\n",
    "ENABLE_CALIBRATOR      = True    # learn/apply per-ticker linear adjustment\n",
    "AUTO_RETRAIN_ON_NEW    = True    # True => if TICKER wasn't in last global training, retrain now\n",
    "RETRAIN_ADD_TO_UNIVERSE= True    # True => append new symbols to TICKERS_FILE before retraining\n",
    "\n",
    "# Global artifacts (shared by ALL symbols)\n",
    "ARTIFACTS_DIR   = Path(\"artifacts\"); ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "GLOBAL_MODEL_M  = ARTIFACTS_DIR / \"global_lstm_multi.keras\"   # HORIZON-step model\n",
    "GLOBAL_MODEL_1  = ARTIFACTS_DIR / \"global_lstm_one.keras\"     # 1-step model\n",
    "GLOBAL_SCALER_X = ARTIFACTS_DIR / \"global_scaler_X.joblib\"\n",
    "GLOBAL_SCALER_Y = ARTIFACTS_DIR / \"global_scaler_Y.joblib\"    # for 1-step returns\n",
    "REGISTRY_PATH   = ARTIFACTS_DIR / \"symbol_index.json\"\n",
    "CALIBRATORS_JSON= ARTIFACTS_DIR / \"calibrators.json\"\n",
    "TRAINED_SET_PATH= ARTIFACTS_DIR / \"global_trained_symbols.json\"  # snapshot of last global training universe\n",
    "\n",
    "# Training universe to build the global model\n",
    "if Path(TICKERS_FILE).exists():\n",
    "    EXAMPLE_TICKERS = [l.strip() for l in Path(TICKERS_FILE).read_text().splitlines() if l.strip()]\n",
    "else:\n",
    "    EXAMPLE_TICKERS = [\n",
    "        \"SPY\",\"QQQ\",\"IWM\",\"EFA\",\"EEM\",\"GLD\",\"SLV\",\"TLT\",\"HYG\",\"XLF\",\"XLE\",\"XLY\",\"XLK\",\"XLV\",\"XLI\",\"XLP\",\"XLB\",\"XLU\",\"VNQ\",\"ARKK\",\n",
    "        \"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\",\"META\",\"TSLA\",\"BRK-B\",\"JPM\",\"V\",\"JNJ\",\"WMT\",\"PG\",\"UNH\",\"MA\",\"HD\",\"XOM\",\"BAC\",\"PFE\",\"DIS\",\n",
    "        \"BTC-USD\",\"ETH-USD\",\"SOL-USD\",\"BNB-USD\",\"XRP-USD\",\"ADA-USD\",\"DOGE-USD\"\n",
    "    ]\n",
    "# Ensure the focus ticker is known in-memory (file update handled later if needed)\n",
    "if TICKER not in EXAMPLE_TICKERS:\n",
    "    EXAMPLE_TICKERS = [TICKER] + EXAMPLE_TICKERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b9eb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Helper functions & GLOBAL model\n",
    "# -------------------------------\n",
    "MAX_SYMBOLS = 20000\n",
    "\n",
    "def _load_registry():\n",
    "    if REGISTRY_PATH.exists():\n",
    "        try: return json.loads(REGISTRY_PATH.read_text())\n",
    "        except Exception: pass\n",
    "    return {\"symbol_to_id\":{}, \"id_to_symbol\":{}}\n",
    "\n",
    "def _save_registry(reg:dict):\n",
    "    REGISTRY_PATH.write_text(json.dumps(reg, indent=2))\n",
    "\n",
    "def _ensure_sid(symbol:str, reg:dict)->int:\n",
    "    s=symbol.upper().strip()\n",
    "    if s in reg[\"symbol_to_id\"]: return reg[\"symbol_to_id\"][s]\n",
    "    nxt = 0\n",
    "    used = set(reg[\"symbol_to_id\"].values())\n",
    "    while nxt in used: nxt += 1\n",
    "    reg[\"symbol_to_id\"][s]=nxt; reg[\"id_to_symbol\"][str(nxt)] = s\n",
    "    _save_registry(reg)\n",
    "    return nxt\n",
    "\n",
    "def _load_calibrators():\n",
    "    if CALIBRATORS_JSON.exists():\n",
    "        try: return json.loads(CALIBRATORS_JSON.read_text())\n",
    "        except Exception: pass\n",
    "    return {}\n",
    "\n",
    "def _save_calibrators(obj:dict):\n",
    "    CALIBRATORS_JSON.write_text(json.dumps(obj, indent=2))\n",
    "\n",
    "def was_in_last_training(symbol:str) -> bool:\n",
    "    try:\n",
    "        symbols = json.loads(TRAINED_SET_PATH.read_text())\n",
    "        return symbol.upper() in {s.upper() for s in symbols}\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def add_symbol_to_universe_file(symbol:str, path=TICKERS_FILE):\n",
    "    p = Path(path)\n",
    "    lines = set()\n",
    "    if p.exists():\n",
    "        lines = {l.strip() for l in p.read_text().splitlines() if l.strip()}\n",
    "    lines.add(symbol.upper())\n",
    "    p.write_text(\"\\n\".join(sorted(lines)))\n",
    "\n",
    "def fetch_prices(ticker: str, start=\"2016-01-01\", end=None, interval=\"1d\") -> pd.DataFrame:\n",
    "    df = yf.download(ticker, start=start, end=end, interval=interval, auto_adjust=True, progress=False, timeout=30)\n",
    "    if df is None or df.empty: raise ValueError(\"No data returned.\")\n",
    "    df.index = pd.to_datetime(df.index); df.index.name = \"Date\"\n",
    "    cols = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"] if c in df.columns]\n",
    "    if \"Close\" not in cols and \"Adj Close\" in df.columns:\n",
    "        df[\"Close\"] = df[\"Adj Close\"]; cols = [*cols, \"Close\"]\n",
    "    return df[cols].dropna()\n",
    "\n",
    "def fetch_prices_auto(ticker: str, interval=\"1d\") -> pd.DataFrame:\n",
    "    for p in [\"1y\",\"150d\",\"90d\",\"60d\",\"30d\"]:\n",
    "        try:\n",
    "            df = yf.download(ticker, period=p, interval=interval, auto_adjust=True, progress=False, timeout=30)\n",
    "            if df is not None and not df.empty:\n",
    "                df = df[[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]].dropna()\n",
    "                if not df.empty:\n",
    "                    df.index = pd.to_datetime(df.index); df.index.name = \"Date\"\n",
    "                    return df\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise ValueError(\"No data for any fallback period.\")\n",
    "\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    close = pd.to_numeric(out[\"Close\"].squeeze(), errors=\"coerce\").where(lambda x: x>0, np.nan)\n",
    "    out[\"Close\"] = close\n",
    "    out[\"log_ret\"] = np.log(close).diff()\n",
    "    out[\"ret\"] = close.pct_change()\n",
    "    out[\"roll_mean_7\"]  = close.rolling(7).mean()\n",
    "    out[\"roll_std_7\"]   = close.rolling(7).std()\n",
    "    out[\"roll_mean_21\"] = close.rolling(21).mean()\n",
    "    out[\"roll_std_21\"]  = close.rolling(21).std()\n",
    "    delta = close.diff(); up = delta.clip(lower=0); down = -delta.clip(upper=0)\n",
    "    roll_up = up.ewm(alpha=1/14, min_periods=14, adjust=False).mean()\n",
    "    roll_dn = down.ewm(alpha=1/14, min_periods=14, adjust=False).mean()\n",
    "    rs = roll_up / roll_dn.replace(0, np.nan)\n",
    "    out[\"rsi_14\"] = 100 - (100 / (1 + rs))\n",
    "    ema12 = close.ewm(span=12, adjust=False).mean()\n",
    "    ema26 = close.ewm(span=26, adjust=False).mean()\n",
    "    macd = ema12 - ema26\n",
    "    out[\"macd\"] = macd\n",
    "    out[\"macd_signal\"] = macd.ewm(span=9, adjust=False).mean()\n",
    "    out[\"macd_diff\"] = out[\"macd\"] - out[\"macd_signal\"]\n",
    "    ma20 = close.rolling(20).mean(); sd20 = close.rolling(20).std()\n",
    "    out[\"bb_width\"] = (ma20 + 2*sd20 - (ma20 - 2*sd20)) / close\n",
    "    out[\"ret_lag1\"] = out[\"log_ret\"].shift(1)\n",
    "    out[\"ret_lag3\"] = out[\"log_ret\"].shift(3)\n",
    "    out[\"ret_lag5\"] = out[\"log_ret\"].shift(5)\n",
    "    out[\"vol_7\"] = out[\"log_ret\"].rolling(7).std()\n",
    "    out[\"vol_21\"] = out[\"log_ret\"].rolling(21).std()\n",
    "    out[\"z_close_21\"] = (close - close.rolling(21).mean()) / close.rolling(21).std()\n",
    "    return out.dropna()\n",
    "\n",
    "FEATURES = [\n",
    "    \"Close\",\"Volume\",\"log_ret\",\"ret\",\n",
    "    \"roll_mean_7\",\"roll_std_7\",\"roll_mean_21\",\"roll_std_21\",\n",
    "    \"rsi_14\",\"macd\",\"macd_signal\",\"macd_diff\",\n",
    "    \"bb_width\",\"ret_lag1\",\"ret_lag3\",\"ret_lag5\",\n",
    "    \"vol_7\",\"vol_21\",\"z_close_21\",\n",
    "]\n",
    "\n",
    "def make_windows(X: np.ndarray, y: np.ndarray, L: int, H: int):\n",
    "    xs, ys = [], []\n",
    "    for i in range(L, len(X) - H + 1):\n",
    "        xs.append(X[i-L:i, :])\n",
    "        ys.append(y[i:i+H])\n",
    "    return np.array(xs, dtype=\"float32\"), np.array(ys, dtype=\"float32\")\n",
    "\n",
    "def build_global_model(input_steps: int, n_features: int, horizon: int) -> tf.keras.Model:\n",
    "    ts_in  = tf.keras.Input(shape=(input_steps, n_features), name=\"ts_in\")\n",
    "    sid_in = tf.keras.Input(shape=(), dtype=\"int32\", name=\"sid_in\")\n",
    "    emb = tf.keras.layers.Embedding(MAX_SYMBOLS, 16, name=\"sym_emb\")(sid_in)\n",
    "    emb_r = tf.keras.layers.RepeatVector(input_steps)(emb)\n",
    "    x = tf.keras.layers.Concatenate()([ts_in, emb_r])\n",
    "    x = tf.keras.layers.Conv1D(48, kernel_size=5, padding=\"causal\", activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(160, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.LSTM(96)(x)\n",
    "    out = tf.keras.layers.Dense(horizon, name=\"step_returns\")(x)\n",
    "    m = tf.keras.Model([ts_in, sid_in], out, name=\"global_lstm_multiasset\")\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=\"mse\")\n",
    "    return m\n",
    "\n",
    "def _fit_scaler_global(X: np.ndarray) -> StandardScaler:\n",
    "    N,L,F = X.shape\n",
    "    sc = StandardScaler().fit(X.reshape(N*L, F))\n",
    "    return sc\n",
    "\n",
    "def _apply_scaler_global(X: np.ndarray, sc: StandardScaler) -> np.ndarray:\n",
    "    N,L,F = X.shape\n",
    "    flat = sc.transform(X.reshape(N*L, F))\n",
    "    return flat.reshape(N, L, F)\n",
    "\n",
    "def _build_dataset_for_universe(tickers:list, lookback:int, horizon:int, interval:str=\"1d\"):\n",
    "    reg = _load_registry()\n",
    "    Xs, ys, sids = [], [], []\n",
    "    for s in tickers:\n",
    "        try:\n",
    "            df = fetch_prices_auto(s, interval=interval)\n",
    "        except Exception:\n",
    "            continue\n",
    "        feat = add_features(df)\n",
    "        X = feat[FEATURES].values.astype(\"float32\")\n",
    "        y = feat[\"ret\"].fillna(0).values.astype(\"float32\")\n",
    "        Xw, Yw = make_windows(X, y, lookback, horizon)\n",
    "        if len(Xw)==0: continue\n",
    "        sid = _ensure_sid(s, reg)\n",
    "        Xs.append(Xw)\n",
    "        ys.append(Yw)\n",
    "        sids.append(np.full((len(Xw),), sid, dtype=\"int32\"))\n",
    "    if not Xs:\n",
    "        raise ValueError(\"No training samples from the provided universe.\")\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    Y = np.concatenate(ys, axis=0)\n",
    "    S = np.concatenate(sids, axis=0)\n",
    "    return X, Y, S, reg\n",
    "\n",
    "def ensure_global_trained(lookback:int, horizon:int, interval:str=\"1d\", force:bool=False):\n",
    "    need_train = force or (not GLOBAL_MODEL_M.exists()) or (not GLOBAL_MODEL_1.exists()) or (not GLOBAL_SCALER_X.exists()) or (not GLOBAL_SCALER_Y.exists())\n",
    "    if not need_train:\n",
    "        return\n",
    "    X, Y, S, reg = _build_dataset_for_universe(EXAMPLE_TICKERS, lookback, horizon, interval=interval)\n",
    "    scX = _fit_scaler_global(X)\n",
    "    Xs  = _apply_scaler_global(X, scX)\n",
    "    # multi-step model\n",
    "    mM = build_global_model(lookback, Xs.shape[-1], horizon)\n",
    "    mM.fit({\"ts_in\":Xs,\"sid_in\":S}, Y, epochs=18, batch_size=256, verbose=1)\n",
    "    mM.save(GLOBAL_MODEL_M)\n",
    "    dump(scX, GLOBAL_SCALER_X)\n",
    "    _save_registry(reg)\n",
    "    # one-step model\n",
    "    Y1 = Y[:, :1]\n",
    "    m1 = build_global_model(lookback, Xs.shape[-1], 1)\n",
    "    m1.fit({\"ts_in\":Xs,\"sid_in\":S}, Y1, epochs=12, batch_size=256, verbose=1)\n",
    "    m1.save(GLOBAL_MODEL_1)\n",
    "    scY = StandardScaler().fit(Y1.reshape(-1,1))\n",
    "    dump(scY, GLOBAL_SCALER_Y)\n",
    "    # snapshot which symbols trained the current global weights\n",
    "    TRAINED_SET_PATH.write_text(json.dumps(sorted(list(set(EXAMPLE_TICKERS))), indent=2))\n",
    "\n",
    "def infer_freq_has_weekends(idx: pd.Index) -> bool:\n",
    "    try:\n",
    "        wd = pd.Index([pd.Timestamp(x).weekday() for x in idx])\n",
    "        return (wd >= 5).any()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def next_dates(last_date, n: int, use_weekends: bool) -> list:\n",
    "    last_ts = pd.Timestamp(last_date)\n",
    "    if use_weekends:\n",
    "        rng = pd.date_range(last_ts, periods=n+1, freq=\"D\"); return [d for d in rng[1:]]\n",
    "    else:\n",
    "        rng = pd.bdate_range(last_ts, periods=n+1); return [d for d in rng[1:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcfee9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['ATVI']: YFPricesMissingError('possibly delisted; no price data found  (period=1y) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['ATVI']: YFPricesMissingError('possibly delisted; no price data found  (period=150d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['ATVI']: YFPricesMissingError('possibly delisted; no price data found  (period=90d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['ATVI']: YFPricesMissingError('possibly delisted; no price data found  (period=60d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['ATVI']: YFPricesMissingError('possibly delisted; no price data found  (period=30d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['PEAK']: YFPricesMissingError('possibly delisted; no price data found  (period=1y) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['PEAK']: YFPricesMissingError('possibly delisted; no price data found  (period=150d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['PEAK']: YFPricesMissingError('possibly delisted; no price data found  (period=90d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['PEAK']: YFPricesMissingError('possibly delisted; no price data found  (period=60d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['PEAK']: YFPricesMissingError('possibly delisted; no price data found  (period=30d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['SIVB']: YFPricesMissingError('possibly delisted; no price data found  (period=1y) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['SIVB']: YFPricesMissingError('possibly delisted; no price data found  (period=150d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['SIVB']: YFPricesMissingError('possibly delisted; no price data found  (period=90d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['SIVB']: YFPricesMissingError('possibly delisted; no price data found  (period=60d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['SIVB']: YFPricesMissingError('possibly delisted; no price data found  (period=30d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['BBBY']: YFPricesMissingError('possibly delisted; no price data found  (period=1y) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['BBBY']: YFPricesMissingError('possibly delisted; no price data found  (period=150d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['BBBY']: YFPricesMissingError('possibly delisted; no price data found  (period=90d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['BBBY']: YFPricesMissingError('possibly delisted; no price data found  (period=60d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['BBBY']: YFPricesMissingError('possibly delisted; no price data found  (period=30d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['IMX-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=1y)')\n",
      "\n",
      "1 Failed download:\n",
      "['IMX-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=150d)')\n",
      "\n",
      "1 Failed download:\n",
      "['IMX-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=90d)')\n",
      "\n",
      "1 Failed download:\n",
      "['IMX-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=60d)')\n",
      "\n",
      "1 Failed download:\n",
      "['IMX-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=30d)')\n",
      "\n",
      "1 Failed download:\n",
      "['GRT-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=1y)')\n",
      "\n",
      "1 Failed download:\n",
      "['GRT-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=150d)')\n",
      "\n",
      "1 Failed download:\n",
      "['GRT-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=90d)')\n",
      "\n",
      "1 Failed download:\n",
      "['GRT-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=60d)')\n",
      "\n",
      "1 Failed download:\n",
      "['GRT-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=30d)')\n",
      "\n",
      "1 Failed download:\n",
      "['SUI-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=1y)')\n",
      "\n",
      "1 Failed download:\n",
      "['SUI-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=150d)')\n",
      "\n",
      "1 Failed download:\n",
      "['SUI-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=90d)')\n",
      "\n",
      "1 Failed download:\n",
      "['SUI-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=60d)')\n",
      "\n",
      "1 Failed download:\n",
      "['SUI-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=30d)')\n",
      "\n",
      "1 Failed download:\n",
      "['RNDR-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=1y)')\n",
      "\n",
      "1 Failed download:\n",
      "['RNDR-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=150d)')\n",
      "\n",
      "1 Failed download:\n",
      "['RNDR-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=90d)')\n",
      "\n",
      "1 Failed download:\n",
      "['RNDR-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=60d)')\n",
      "\n",
      "1 Failed download:\n",
      "['RNDR-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=30d)')\n",
      "\n",
      "1 Failed download:\n",
      "['COMP-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=1y)')\n",
      "\n",
      "1 Failed download:\n",
      "['COMP-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=150d)')\n",
      "\n",
      "1 Failed download:\n",
      "['COMP-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=90d)')\n",
      "\n",
      "1 Failed download:\n",
      "['COMP-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=60d)')\n",
      "\n",
      "1 Failed download:\n",
      "['COMP-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=30d)')\n",
      "\n",
      "1 Failed download:\n",
      "['LUNA1-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=1y)')\n",
      "\n",
      "1 Failed download:\n",
      "['LUNA1-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=150d)')\n",
      "\n",
      "1 Failed download:\n",
      "['LUNA1-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=90d)')\n",
      "\n",
      "1 Failed download:\n",
      "['LUNA1-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=60d)')\n",
      "\n",
      "1 Failed download:\n",
      "['LUNA1-USD']: YFPricesMissingError('possibly delisted; no price data found  (period=30d)')\n",
      "\n",
      "1 Failed download:\n",
      "['STOMCA']: YFPricesMissingError('possibly delisted; no price data found  (period=1y) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['STOMCA']: YFPricesMissingError('possibly delisted; no price data found  (period=150d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['STOMCA']: YFPricesMissingError('possibly delisted; no price data found  (period=90d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['STOMCA']: YFPricesMissingError('possibly delisted; no price data found  (period=60d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['STOMCA']: YFPricesMissingError('possibly delisted; no price data found  (period=30d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['AKDD']: YFPricesMissingError('possibly delisted; no price data found  (period=1y) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['AKDD']: YFPricesMissingError('possibly delisted; no price data found  (period=150d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['AKDD']: YFPricesMissingError('possibly delisted; no price data found  (period=90d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['AKDD']: YFPricesMissingError('possibly delisted; no price data found  (period=60d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "\n",
      "1 Failed download:\n",
      "['AKDD']: YFPricesMissingError('possibly delisted; no price data found  (period=30d) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 162ms/step - loss: 971452.2500\n",
      "Epoch 2/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 170ms/step - loss: 971427.1250\n",
      "Epoch 3/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 180ms/step - loss: 971411.8125\n",
      "Epoch 4/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 179ms/step - loss: 971398.0625\n",
      "Epoch 5/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 182ms/step - loss: 971397.1875\n",
      "Epoch 6/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 184ms/step - loss: 971392.5625\n",
      "Epoch 7/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 186ms/step - loss: 971377.1250\n",
      "Epoch 8/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 188ms/step - loss: 971365.8750\n",
      "Epoch 9/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 189ms/step - loss: 971358.4375\n",
      "Epoch 10/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 198ms/step - loss: 971344.6250\n",
      "Epoch 11/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 198ms/step - loss: 971338.3125\n",
      "Epoch 12/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 198ms/step - loss: 971330.3125\n",
      "Epoch 13/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 199ms/step - loss: 971318.3750\n",
      "Epoch 14/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 203ms/step - loss: 971306.4375\n",
      "Epoch 15/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 205ms/step - loss: 971293.9375\n",
      "Epoch 16/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 216ms/step - loss: 971288.0000\n",
      "Epoch 17/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 235ms/step - loss: 971285.6250\n",
      "Epoch 18/18\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 222ms/step - loss: 971264.8750\n",
      "Epoch 1/12\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 206ms/step - loss: 971445.9375\n",
      "Epoch 2/12\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 209ms/step - loss: 971396.8750\n",
      "Epoch 3/12\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 205ms/step - loss: 971383.8125\n",
      "Epoch 4/12\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 204ms/step - loss: 971372.4375\n",
      "Epoch 5/12\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 205ms/step - loss: 971345.9375\n",
      "Epoch 6/12\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 205ms/step - loss: 971347.6250\n",
      "Epoch 7/12\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 206ms/step - loss: 971317.3750\n",
      "Epoch 8/12\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 211ms/step - loss: 971338.0625\n",
      "Epoch 9/12\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 206ms/step - loss: 971320.3125\n",
      "Epoch 10/12\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 208ms/step - loss: 971313.7500\n",
      "Epoch 11/12\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 213ms/step - loss: 971304.1875\n",
      "Epoch 12/12\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 217ms/step - loss: 971287.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price   = float(close_np[end_idx-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  actual_price = float(close_np[end_idx])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:82: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  prev_price = float(close_np[train_end_idx - 1 + k])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:83: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  cur_price  = float(close_np[train_end_idx + k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WROTE JSON => forecast.json\n",
      "In last global training set: True\n",
      "Has calibrator now: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:106: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  last_price = float(close_np[-1])\n",
      "/var/folders/vx/b0mk9dxj6cj0p22ynsbn1b440000gn/T/ipykernel_4843/79452759.py:115: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  row = series_map.get(ds, {\"date\": ds}); row[\"actual\"] = float(p); row.setdefault(\"part\", \"context\"); series_map[ds] = row\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Main logic (GLOBAL model + per-ticker calibrator)\n",
    "# -------------------------------\n",
    "\n",
    "# 0) If this is a brand-new symbol vs last global training, optionally retrain now\n",
    "is_new_to_global = not was_in_last_training(TICKER)\n",
    "if AUTO_RETRAIN_ON_NEW and is_new_to_global:\n",
    "    if RETRAIN_ADD_TO_UNIVERSE:\n",
    "        add_symbol_to_universe_file(TICKER, path=TICKERS_FILE)\n",
    "    if TICKER not in EXAMPLE_TICKERS:\n",
    "        EXAMPLE_TICKERS = [TICKER] + EXAMPLE_TICKERS\n",
    "    ensure_global_trained(LOOKBACK, HORIZON, interval=INTERVAL, force=True)\n",
    "else:\n",
    "    ensure_global_trained(LOOKBACK, HORIZON, interval=INTERVAL, force=FORCE_RETRAIN_GLOBAL)\n",
    "\n",
    "# 1) Pull enough data for the single target TICKER (smart fallback)\n",
    "buffer_days = 320\n",
    "start_date = (pd.Timestamp.utcnow() - pd.Timedelta(days=CONTEXT + LOOKBACK + BACKTEST_HORIZON + buffer_days)).date().isoformat()\n",
    "try:\n",
    "    raw = fetch_prices(TICKER, start=start_date, interval=INTERVAL)\n",
    "    df = add_features(raw)\n",
    "    if len(df) < (LOOKBACK + BACKTEST_HORIZON + CONTEXT + 10):\n",
    "        raise ValueError(\"fallback\")\n",
    "except Exception:\n",
    "    raw = fetch_prices_auto(TICKER, interval=INTERVAL)\n",
    "    df = add_features(raw)\n",
    "\n",
    "df[\"target_ret\"] = df[\"log_ret\"].shift(-1)\n",
    "df = df.dropna()\n",
    "if len(df) < LOOKBACK + BACKTEST_HORIZON + 5:\n",
    "    raise ValueError(\"Not enough rows after feature engineering.\")\n",
    "\n",
    "# 2) Keep just the window we need for context + train + backtest slice (for calibrator)\n",
    "df_tail = df.tail(CONTEXT + LOOKBACK + BACKTEST_HORIZON)\n",
    "X_df = df_tail[FEATURES].astype(\"float32\")\n",
    "y = df_tail[\"target_ret\"].astype(\"float32\").values\n",
    "dates = df_tail.index\n",
    "\n",
    "# split point for backtest\n",
    "train_end_idx = len(df_tail) - BACKTEST_HORIZON\n",
    "if train_end_idx <= LOOKBACK:\n",
    "    train_end_idx = LOOKBACK + 1\n",
    "    BACKTEST_HORIZON = max(1, len(df_tail) - train_end_idx)\n",
    "\n",
    "X_train_df = X_df.iloc[:train_end_idx]\n",
    "y_train = y[:train_end_idx]\n",
    "n_features = X_train_df.shape[1]\n",
    "\n",
    "# 3) Load global scalers/models\n",
    "scaler_X = load(GLOBAL_SCALER_X)\n",
    "scaler_Y = load(GLOBAL_SCALER_Y)\n",
    "m_multi  = tf.keras.models.load_model(GLOBAL_MODEL_M)\n",
    "m_one    = tf.keras.models.load_model(GLOBAL_MODEL_1)\n",
    "\n",
    "# Symbol id for the target ticker\n",
    "reg = _load_registry()\n",
    "sid = _ensure_sid(TICKER, reg)\n",
    "\n",
    "# 4) Backtest last BACKTEST_HORIZON days (1-step each) to update per-ticker calibrator\n",
    "backtest_dates, backtest_pred_prices, backtest_actual_prices, backtest_pred_rets = [], [], [], []\n",
    "close_np = df_tail[\"Close\"].to_numpy()\n",
    "\n",
    "for k in range(int(BACKTEST_HORIZON)):\n",
    "    end_idx = train_end_idx + k\n",
    "    xb_raw = X_df.values[end_idx-LOOKBACK:end_idx, :]\n",
    "    xb = scaler_X.transform(xb_raw).reshape(1, LOOKBACK, n_features)\n",
    "    y1_s = m_one.predict({\"ts_in\":xb, \"sid_in\":np.array([sid])}, verbose=0)[0]  # (1,)\n",
    "    pred_ret = scaler_Y.inverse_transform(y1_s.reshape(-1,1))[0,0]\n",
    "    last_price   = float(close_np[end_idx-1])\n",
    "    pred_price   = float(last_price * np.exp(pred_ret))\n",
    "    actual_price = float(close_np[end_idx])\n",
    "    backtest_dates.append(pd.Timestamp(dates[end_idx]).strftime(\"%Y-%m-%d\"))\n",
    "    backtest_pred_prices.append(pred_price)\n",
    "    backtest_actual_prices.append(actual_price)\n",
    "    backtest_pred_rets.append(float(pred_ret))\n",
    "\n",
    "rmse = float(np.sqrt(mean_squared_error(backtest_actual_prices, backtest_pred_prices)))\n",
    "mape = float(mean_absolute_percentage_error(backtest_actual_prices, backtest_pred_prices) * 100)\n",
    "\n",
    "actual_rets = []\n",
    "for k in range(int(BACKTEST_HORIZON)):\n",
    "    prev_price = float(close_np[train_end_idx - 1 + k])\n",
    "    cur_price  = float(close_np[train_end_idx + k])\n",
    "    actual_rets.append((cur_price - prev_price) / (prev_price if prev_price != 0 else 1.0))\n",
    "acc = float((np.sign(backtest_pred_rets) == np.sign(actual_rets)).mean() * 100.0) if backtest_pred_rets and actual_rets else 0.0\n",
    "\n",
    "# 5) Multi-step forward forecast of next HORIZON sessions + optional per-ticker calibrator\n",
    "last_block_raw = X_df.values[-LOOKBACK:]\n",
    "last_block = scaler_X.transform(last_block_raw).reshape(1, LOOKBACK, n_features)\n",
    "next_rets_s = m_multi.predict({\"ts_in\":last_block, \"sid_in\":np.array([sid])}, verbose=0)[0]  # (HORIZON,)\n",
    "next_rets = next_rets_s.astype(\"float64\")  # multi-step model learned raw returns\n",
    "\n",
    "if ENABLE_CALIBRATOR:\n",
    "    key = TICKER.upper()\n",
    "    cals = _load_calibrators()\n",
    "    if len(backtest_pred_rets) >= 5:\n",
    "        A = np.vstack([backtest_pred_rets, np.ones(len(backtest_pred_rets))]).T\n",
    "        sol, *_ = np.linalg.lstsq(A, np.array(actual_rets[:len(backtest_pred_rets)]), rcond=None)\n",
    "        a, b = float(sol[0]), float(sol[1])\n",
    "        cals[key] = {\"a\": a, \"b\": b}\n",
    "        _save_calibrators(cals)\n",
    "    if key in cals:\n",
    "        a, b = cals[key][\"a\"], cals[key][\"b\"]\n",
    "        next_rets = a * next_rets + b\n",
    "\n",
    "last_price = float(close_np[-1])\n",
    "future_pred_prices = (last_price * np.exp(np.cumsum(next_rets))).astype(float)\n",
    "expected_move_pct = float((float(future_pred_prices[-1]) - last_price) / (last_price if last_price != 0 else 1.0) * 100.0)\n",
    "\n",
    "# 6) Build unified series for frontend (context actuals + backtest preds + future preds)\n",
    "series_map = {}\n",
    "ctx_tail = df_tail.tail(int(CONTEXT))\n",
    "for d, p in zip(ctx_tail.index, ctx_tail[\"Close\"].astype(float).values):\n",
    "    ds = pd.Timestamp(d).strftime(\"%Y-%m-%d\")\n",
    "    row = series_map.get(ds, {\"date\": ds}); row[\"actual\"] = float(p); row.setdefault(\"part\", \"context\"); series_map[ds] = row\n",
    "\n",
    "for i, dstr in enumerate(backtest_dates[:len(backtest_pred_prices)]):\n",
    "    row = series_map.get(dstr, {\"date\": dstr}); row[\"pred\"] = float(backtest_pred_prices[i]); row[\"part\"] = \"backtest\"; series_map[dstr] = row\n",
    "\n",
    "has_weekends = infer_freq_has_weekends(df_tail.index)\n",
    "future_dates = next_dates(df_tail.index[-1], int(HORIZON), use_weekends=has_weekends)\n",
    "for i, p in enumerate(future_pred_prices):\n",
    "    ds = pd.Timestamp(future_dates[i]).strftime(\"%Y-%m-%d\")\n",
    "    row = series_map.get(ds, {\"date\": ds}); row[\"pred\"] = float(p); row[\"part\"] = \"forecast\"; series_map[ds] = row\n",
    "\n",
    "series = sorted(series_map.values(), key=lambda r: r[\"date\"])\n",
    "metrics = {\"rmse\": float(rmse), \"mape\": float(mape), \"accuracy_pct\": float(acc), \"expected_10d_move_pct\": float(expected_move_pct)}\n",
    "result = {\n",
    "  \"ticker\": str(TICKER).upper(),\n",
    "  \"interval\": INTERVAL,\n",
    "  \"look_back\": int(LOOKBACK),\n",
    "  \"context\": int(CONTEXT),\n",
    "  \"backtest_horizon\": int(BACKTEST_HORIZON),\n",
    "  \"horizon\": int(HORIZON),\n",
    "  \"metrics\": metrics,\n",
    "  \"forecast\": series,\n",
    "}\n",
    "\n",
    "out_path = str(OUTPUT_JSON)\n",
    "os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(result, f)\n",
    "print(\"WROTE JSON =>\", out_path)\n",
    "print(\"In last global training set:\", was_in_last_training(TICKER))\n",
    "print(\"Has calibrator now:\", TICKER.upper() in _load_calibrators())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87c6c86-b2b9-4a7f-81c1-937762c80b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
