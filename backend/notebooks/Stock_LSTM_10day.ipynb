{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20941032",
   "metadata": {},
   "source": [
    "# Stock LSTM — Unified Backtest + 10‑day Forecast\n",
    "\n",
    "This notebook is parameterized for Papermill.\n",
    "\n",
    "**Inputs:** `TICKER, LOOKBACK, CONTEXT, BACKTEST_HORIZON, HORIZON, OUTPUT_JSON`\n",
    "\n",
    "**Output file format (JSON)**:\n",
    "```json\n",
    "{\n",
    "  \"ticker\": \"AAPL\",\n",
    "  \"look_back\": 60,\n",
    "  \"context\": 100,\n",
    "  \"backtest_horizon\": 20,\n",
    "  \"horizon\": 10,\n",
    "  \"metrics\": {\n",
    "    \"rmse\": 0.0,\n",
    "    \"mape\": 0.0,\n",
    "    \"accuracy_pct\": 0.0,\n",
    "    \"expected_10d_move_pct\": 0.0\n",
    "  },\n",
    "  \"forecast\": [\n",
    "    {\"date\": \"YYYY-MM-DD\", \"actual\": 123.45, \"part\": \"context\"},\n",
    "    {\"date\": \"YYYY-MM-DD\", \"pred\": 123.45, \"part\": \"backtest\"},\n",
    "    {\"date\": \"YYYY-MM-DD\", \"pred\": 123.45, \"part\": \"forecast\"}\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddf3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c31424",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill parameters (will be overridden when executed by Papermill)\n",
    "TICKER = \"AAPL\"\n",
    "LOOKBACK = 60\n",
    "CONTEXT = 100\n",
    "BACKTEST_HORIZON = 20\n",
    "HORIZON = 10\n",
    "OUTPUT_JSON = \"forecast.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9eb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Helper functions & model\n",
    "# -------------------------------\n",
    "\n",
    "def fetch_prices(ticker: str, start=\"2016-01-01\", end=None, interval=\"1d\") -> pd.DataFrame:\n",
    "    df = yf.download(ticker, start=start, end=end, interval=interval, auto_adjust=True, progress=False)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No data returned. Check ticker/interval or your network.\")\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df.index.name = \"Date\"\n",
    "    return df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].dropna()\n",
    "\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    close = pd.to_numeric(out[\"Close\"].squeeze(), errors=\"coerce\")\n",
    "    close = close.where(close > 0, np.nan)\n",
    "    out[\"Close\"] = close\n",
    "\n",
    "    # base returns\n",
    "    out[\"log_ret\"] = np.log(close).diff()\n",
    "    out[\"ret\"] = close.pct_change()\n",
    "\n",
    "    # rolling stats\n",
    "    out[\"roll_mean_7\"] = close.rolling(7).mean()\n",
    "    out[\"roll_std_7\"] = close.rolling(7).std()\n",
    "    out[\"roll_mean_21\"] = close.rolling(21).mean()\n",
    "    out[\"roll_std_21\"] = close.rolling(21).std()\n",
    "\n",
    "    # RSI(14)\n",
    "    delta = close.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -delta.clip(upper=0)\n",
    "    roll_up = up.ewm(alpha=1 / 14, min_periods=14, adjust=False).mean()\n",
    "    roll_dn = down.ewm(alpha=1 / 14, min_periods=14, adjust=False).mean()\n",
    "    rs = roll_up / roll_dn.replace(0, np.nan)\n",
    "    out[\"rsi_14\"] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # MACD (12,26,9)\n",
    "    ema12 = close.ewm(span=12, adjust=False).mean()\n",
    "    ema26 = close.ewm(span=26, adjust=False).mean()\n",
    "    macd = ema12 - ema26\n",
    "    out[\"macd\"] = macd\n",
    "    out[\"macd_signal\"] = macd.ewm(span=9, adjust=False).mean()\n",
    "    out[\"macd_diff\"] = out[\"macd\"] - out[\"macd_signal\"]\n",
    "\n",
    "    # Bollinger width (20,2)\n",
    "    ma20 = close.rolling(20).mean()\n",
    "    sd20 = close.rolling(20).std()\n",
    "    out[\"bb_width\"] = (ma20 + 2 * sd20 - (ma20 - 2 * sd20)) / close\n",
    "\n",
    "    # lags & volatility\n",
    "    out[\"ret_lag1\"] = out[\"log_ret\"].shift(1)\n",
    "    out[\"ret_lag3\"] = out[\"log_ret\"].shift(3)\n",
    "    out[\"ret_lag5\"] = out[\"log_ret\"].shift(5)\n",
    "    out[\"vol_7\"] = out[\"log_ret\"].rolling(7).std()\n",
    "    out[\"vol_21\"] = out[\"log_ret\"].rolling(21).std()\n",
    "    out[\"z_close_21\"] = (close - close.rolling(21).mean()) / close.rolling(21).std()\n",
    "\n",
    "    return out.dropna()\n",
    "\n",
    "FEATURES = [\n",
    "    \"Close\",\"Volume\",\"log_ret\",\"ret\",\n",
    "    \"roll_mean_7\",\"roll_std_7\",\"roll_mean_21\",\"roll_std_21\",\n",
    "    \"rsi_14\",\"macd\",\"macd_signal\",\"macd_diff\",\n",
    "    \"bb_width\",\"ret_lag1\",\"ret_lag3\",\"ret_lag5\",\n",
    "    \"vol_7\",\"vol_21\",\"z_close_21\",\n",
    "]\n",
    "\n",
    "def make_windows(X: np.ndarray, y: np.ndarray, lookback: int, horizon: int):\n",
    "    xs, ys = [], []\n",
    "    for i in range(lookback, len(X) - horizon + 1):\n",
    "        xs.append(X[i - lookback : i, :])\n",
    "        ys.append(y[i : i + horizon])\n",
    "    return np.array(xs, dtype=\"float32\"), np.array(ys, dtype=\"float32\")\n",
    "\n",
    "def build_model(input_steps: int, n_features: int, horizon: int) -> tf.keras.Model:\n",
    "    inp = tf.keras.Input(shape=(input_steps, n_features))\n",
    "    x = tf.keras.layers.Conv1D(48, kernel_size=5, padding=\"causal\", activation=\"relu\")(inp)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(160, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.LSTM(96)(x)\n",
    "    out = tf.keras.layers.Dense(horizon)(x)\n",
    "    model = tf.keras.Model(inp, out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def infer_freq_has_weekends(idx: pd.Index) -> bool:\n",
    "    # If any timestamp lands on Saturday/Sunday, assume weekend series (e.g., crypto)\n",
    "    try:\n",
    "        weekdays = pd.Index([pd.Timestamp(x).weekday() for x in idx])\n",
    "        return ((weekdays >= 5).any())\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def next_dates(last_date, n: int, use_weekends: bool) -> list:\n",
    "    last_ts = pd.Timestamp(last_date)\n",
    "    if use_weekends:\n",
    "        rng = pd.date_range(last_ts, periods=n+1, freq=\"D\")\n",
    "        return [d for d in rng[1:]]\n",
    "    else:\n",
    "        rng = pd.bdate_range(last_ts, periods=n+1)\n",
    "        return [d for d in rng[1:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfee9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Main logic\n",
    "# -------------------------------\n",
    "\n",
    "# 1) Pull enough data\n",
    "buffer_days = 320\n",
    "start_date = (pd.Timestamp.utcnow() - pd.Timedelta(days=CONTEXT + LOOKBACK + BACKTEST_HORIZON + buffer_days)).date().isoformat()\n",
    "raw = fetch_prices(TICKER, start=start_date)\n",
    "df = add_features(raw)\n",
    "df[\"target_ret\"] = df[\"log_ret\"].shift(-1)\n",
    "df = df.dropna()\n",
    "if len(df) < LOOKBACK + BACKTEST_HORIZON + 5:\n",
    "    raise ValueError(\"Not enough rows after feature engineering.\")\n",
    "\n",
    "# 2) Keep just the window we need for context + train + backtest\n",
    "df_tail = df.tail(CONTEXT + LOOKBACK + BACKTEST_HORIZON)\n",
    "X_df = df_tail[FEATURES].astype(\"float32\")\n",
    "y = df_tail[\"target_ret\"].astype(\"float32\").values\n",
    "dates = df_tail.index\n",
    "\n",
    "# split point for backtest\n",
    "train_end_idx = len(df_tail) - BACKTEST_HORIZON\n",
    "if train_end_idx <= LOOKBACK:\n",
    "    train_end_idx = LOOKBACK + 1\n",
    "    BACKTEST_HORIZON = max(1, len(df_tail) - train_end_idx)\n",
    "\n",
    "X_train_df = X_df.iloc[:train_end_idx]\n",
    "y_train = y[:train_end_idx]\n",
    "\n",
    "# 3) Scalers on train only\n",
    "scaler_X = StandardScaler().fit(X_train_df.values)\n",
    "scaler_y = StandardScaler().fit(y_train.reshape(-1,1))\n",
    "X_train = scaler_X.transform(X_train_df.values)\n",
    "y_train_s = scaler_y.transform(y_train.reshape(-1,1)).ravel()\n",
    "\n",
    "# 4) Train model for HORIZON-step forecast\n",
    "Xw, yw = make_windows(X_train, y_train_s, LOOKBACK, HORIZON)\n",
    "if len(Xw) == 0:\n",
    "    raise ValueError(\"No training windows for multi-step model.\")\n",
    "m_multi = build_model(LOOKBACK, Xw.shape[2], HORIZON)\n",
    "cbs=[tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=6, restore_best_weights=True)]\n",
    "m_multi.fit(Xw, yw, epochs=24, batch_size=32, verbose=0, callbacks=cbs)\n",
    "\n",
    "# 5) Train a second model for 1-step backtest\n",
    "Xw1, yw1 = make_windows(X_train, y_train_s, LOOKBACK, 1)\n",
    "if len(Xw1) == 0:\n",
    "    raise ValueError(\"No training windows for 1-step model.\")\n",
    "m_one = build_model(LOOKBACK, Xw1.shape[2], 1)\n",
    "m_one.fit(Xw1, yw1, epochs=16, batch_size=32, verbose=0, callbacks=cbs)\n",
    "\n",
    "# 6) Backtest last BACKTEST_HORIZON days (1-step each)\n",
    "backtest_dates = []\n",
    "backtest_pred_prices = []\n",
    "backtest_actual_prices = []\n",
    "backtest_pred_rets = []\n",
    "for k in range(int(BACKTEST_HORIZON)):\n",
    "    end_idx = train_end_idx + k  # index of the day we're predicting\n",
    "    xb_raw = X_df.values[end_idx-LOOKBACK:end_idx, :]\n",
    "    xb = scaler_X.transform(xb_raw).reshape(1, LOOKBACK, Xw1.shape[2])\n",
    "    pred_ret_s = m_one.predict(xb, verbose=0)[0][0]\n",
    "    pred_ret = scaler_y.inverse_transform([[pred_ret_s]])[0,0]\n",
    "\n",
    "    last_price = float(df_tail[\"Close\"].iloc[end_idx-1])\n",
    "    pred_price = float(last_price * np.exp(pred_ret))\n",
    "    actual_price = float(df_tail[\"Close\"].iloc[end_idx])\n",
    "\n",
    "    backtest_dates.append(pd.Timestamp(dates[end_idx]).strftime(\"%Y-%m-%d\"))\n",
    "    backtest_pred_prices.append(pred_price)\n",
    "    backtest_actual_prices.append(actual_price)\n",
    "    backtest_pred_rets.append(float(pred_ret))\n",
    "\n",
    "# Metrics on backtest\n",
    "rmse = float(np.sqrt(mean_squared_error(backtest_actual_prices, backtest_pred_prices)))\n",
    "mape = float(mean_absolute_percentage_error(backtest_actual_prices, backtest_pred_prices) * 100)\n",
    "\n",
    "actual_rets = []\n",
    "for k in range(int(BACKTEST_HORIZON)):\n",
    "    prev_price = float(df_tail[\"Close\"].iloc[train_end_idx - 1 + k])\n",
    "    cur_price = float(df_tail[\"Close\"].iloc[train_end_idx + k])\n",
    "    actual_rets.append((cur_price - prev_price) / (prev_price if prev_price != 0 else 1.0))\n",
    "\n",
    "# directional accuracy (% of correct up/down)\n",
    "if len(backtest_pred_rets) and len(actual_rets):\n",
    "    acc = float((np.sign(backtest_pred_rets) == np.sign(actual_rets)).mean() * 100.0)\n",
    "else:\n",
    "    acc = 0.0\n",
    "\n",
    "# 7) Multi-step forward forecast of next HORIZON sessions\n",
    "last_block_raw = X_df.values[-LOOKBACK:]\n",
    "last_block = scaler_X.transform(last_block_raw).reshape(1, LOOKBACK, Xw.shape[2])\n",
    "next_rets_s = m_multi.predict(last_block, verbose=0)[0]\n",
    "next_rets = scaler_y.inverse_transform(next_rets_s.reshape(-1,1)).ravel()\n",
    "last_price = float(df_tail[\"Close\"].iloc[-1])\n",
    "future_pred_prices = (last_price * np.exp(np.cumsum(next_rets))).astype(float)\n",
    "expected_move_pct = float((float(future_pred_prices[-1]) - last_price) / (last_price if last_price != 0 else 1.0) * 100.0)\n",
    "\n",
    "# 8) Build unified series for frontend (context actuals + backtest preds + future preds)\n",
    "series_map = {}\n",
    "\n",
    "# Context actuals\n",
    "ctx_tail = df_tail.tail(int(CONTEXT))\n",
    "for d, p in zip(ctx_tail.index, ctx_tail[\"Close\"].astype(float).values):\n",
    "    date_str = pd.Timestamp(d).strftime(\"%Y-%m-%d\")\n",
    "    row = series_map.get(date_str, {\"date\": date_str})\n",
    "    row[\"actual\"] = float(p)\n",
    "    # Only mark part as context if not already labeled by backtest\n",
    "    row.setdefault(\"part\", \"context\")\n",
    "    series_map[date_str] = row\n",
    "\n",
    "# Backtest predictions\n",
    "for i, dstr in enumerate(backtest_dates[:len(backtest_pred_prices)]):\n",
    "    date_str = pd.Timestamp(dstr).strftime(\"%Y-%m-%d\")\n",
    "    row = series_map.get(date_str, {\"date\": date_str})\n",
    "    row[\"pred\"] = float(backtest_pred_prices[i])\n",
    "    row[\"part\"] = \"backtest\"  # mark segment\n",
    "    series_map[date_str] = row\n",
    "\n",
    "# Future predictions\n",
    "has_weekends = infer_freq_has_weekends(df_tail.index)\n",
    "future_dates = next_dates(df_tail.index[-1], int(HORIZON), use_weekends=has_weekends)\n",
    "for i, d in enumerate(future_dates[:len(future_pred_prices)]):\n",
    "    date_str = pd.Timestamp(d).strftime(\"%Y-%m-%d\")\n",
    "    row = series_map.get(date_str, {\"date\": date_str})\n",
    "    row[\"pred\"] = float(future_pred_prices[i])\n",
    "    row[\"part\"] = \"forecast\"\n",
    "    series_map[date_str] = row\n",
    "\n",
    "# Final series sorted by date\n",
    "series = sorted(series_map.values(), key=lambda r: r[\"date\"])\n",
    "\n",
    "metrics = {\n",
    "    \"rmse\": float(rmse),\n",
    "    \"mape\": float(mape),\n",
    "    \"accuracy_pct\": float(acc),\n",
    "    \"expected_10d_move_pct\": float(expected_move_pct),\n",
    "}\n",
    "\n",
    "result = {\n",
    "  \"ticker\": str(TICKER).upper(),\n",
    "  \"look_back\": int(LOOKBACK),\n",
    "  \"context\": int(CONTEXT),\n",
    "  \"backtest_horizon\": int(BACKTEST_HORIZON),\n",
    "  \"horizon\": int(HORIZON),\n",
    "  \"metrics\": metrics,\n",
    "  \"forecast\": series,\n",
    "}\n",
    "\n",
    "# Write JSON\n",
    "out_path = str(OUTPUT_JSON)\n",
    "out_dir = os.path.dirname(out_path) or \".\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(result, f)\n",
    "print(\"WROTE JSON =>\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
