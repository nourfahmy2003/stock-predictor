{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49aefdb4",
   "metadata": {},
   "source": [
    "\n",
    "# ✅ Global LSTM (10‑Day Direction) with Per‑Ticker Calibrators\n",
    "\n",
    "This notebook trains a **single global LSTM** across a mixed universe (stocks/ETFs/crypto) to predict **10‑day direction (up/down)**, **saves** the model + scaler, and then fits **tiny per‑ticker logistic calibrators** (Platt scaling) on the most recent N months to convert the global score/logit into a **ticker‑specific probability**.\n",
    "\n",
    "### What you'll get\n",
    "- `models/global/lstm_model.keras` – the **global** Keras model\n",
    "- `models/global/scaler.pkl` – `StandardScaler` used to scale features\n",
    "- `models/calibrators/<TICKER>.json` – `{A,B}` params for each ticker's calibration\n",
    "- A test cell that outputs a **forecast payload** compatible with your current API/FE shape\n",
    "\n",
    "> You can run this nightly (or convert to a Python script later). The frontend **does not need to change**—you keep returning the same `forecast` list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f714b8",
   "metadata": {},
   "source": [
    "## 0) Setup & Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b8d41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTEBOOK_DIR: /Users/nourf/Projects/stock-predictor/backend/notebooks\n",
      "ensured: /Users/nourf/Projects/stock-predictor/backend/notebooks/models exists? True\n",
      "ensured: /Users/nourf/Projects/stock-predictor/backend/notebooks/models/global exists? True\n",
      "ensured: /Users/nourf/Projects/stock-predictor/backend/notebooks/models/calibrators exists? True\n",
      "MODEL_PATH: /Users/nourf/Projects/stock-predictor/backend/notebooks/models/global/lstm_model.keras\n",
      "SCALER_PATH: /Users/nourf/Projects/stock-predictor/backend/notebooks/models/global/scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- If needed in a fresh environment, UNCOMMENT the installs below ---\n",
    "# Linux/Windows (CPU):\n",
    "# !pip install tensorflow==2.16.1\n",
    "#\n",
    "# Apple Silicon (M1/M2/M3):\n",
    "# !pip install tensorflow-macos==2.16.1 tensorflow-metal==1.1.0\n",
    "#\n",
    "# Common deps:\n",
    "# !pip install yfinance==0.2.43 pandas==2.2.3 numpy==1.26.4 scikit-learn==1.5.2 joblib==1.4.2 Pillow==10.4.0\n",
    "\n",
    "import os, json, joblib, io, base64, math, warnings, random\n",
    "from typing import List, Tuple, Dict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Determinism (best-effort)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Paths\n",
    "try:\n",
    "    # Works if running as a .py script\n",
    "    NOTEBOOK_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # Fallback for Jupyter: get the absolute path of the notebook itself\n",
    "    import pathlib\n",
    "    NOTEBOOK_DIR = str(pathlib.Path().resolve())\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "\n",
    "MODELS_DIR = os.path.join(NOTEBOOK_DIR, \"models\")\n",
    "GLOBAL_DIR = os.path.join(MODELS_DIR, \"global\")\n",
    "CALIB_DIR  = os.path.join(MODELS_DIR, \"calibrators\")\n",
    "\n",
    "for p in [MODELS_DIR, GLOBAL_DIR, CALIB_DIR]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    print(\"ensured:\", p, \"exists?\", os.path.isdir(p))\n",
    "\n",
    "MODEL_PATH  = os.path.join(GLOBAL_DIR, \"lstm_model.keras\")\n",
    "SCALER_PATH = os.path.join(GLOBAL_DIR, \"scaler.pkl\")\n",
    "print(\"MODEL_PATH:\", MODEL_PATH)\n",
    "print(\"SCALER_PATH:\", SCALER_PATH)\n",
    "# Knobs\n",
    "LOOK_BACK = 60           # sequence length (days)\n",
    "HORIZON   = 10           # predict 10-day direction\n",
    "YEARS     = 5            # history to download\n",
    "CAL_WINDOW_MONTHS = 9    # window for per-ticker calibration\n",
    "BATCH = 256\n",
    "EPOCHS = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2988900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /Users/nourf/Projects/stock-predictor/backend/notebooks\n",
      "MODEL_PATH: /Users/nourf/Projects/stock-predictor/backend/notebooks/models/global/lstm_model.keras\n",
      "File exists? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"MODEL_PATH:\", MODEL_PATH)\n",
    "print(\"File exists?\", os.path.exists(MODEL_PATH))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187dc37",
   "metadata": {},
   "source": [
    "## 1) Universe (edit as you like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e073182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universe size: 80\n",
      "Sample: ['AAPL', 'ABNB', 'ADA-USD', 'ADBE', 'AMD', 'AMZN', 'ASML', 'AVAX-USD', 'BA', 'BKNG', 'BNB-USD', 'BRK-B']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "STOCKS = [\n",
    "    \"AAPL\",\"MSFT\",\"NVDA\",\"AMZN\",\"GOOGL\",\"META\",\"TSLA\",\"BRK-B\",\"JPM\",\"V\",\"MA\",\"UNH\",\"JNJ\",\"WMT\",\"PG\",\"HD\",\n",
    "    \"KO\",\"PEP\",\"MRK\",\"PFE\",\"ORCL\",\"CSCO\",\"ADBE\",\"CRM\",\"AMD\",\"INTC\",\"NFLX\",\"COST\",\"NKE\",\"DIS\",\"TSM\",\"ASML\",\n",
    "    \"SAP\",\"TM\",\"BA\",\"CAT\",\"GE\",\"HON\",\"UPS\",\"MCD\",\"SBUX\",\"BKNG\",\"ABNB\",\"UBER\",\"SNOW\",\"SHOP\",\"SQ\",\"PYPL\"\n",
    "]\n",
    "ETFS = [\n",
    "    \"SPY\",\"QQQ\",\"DIA\",\"IWM\",\"EEM\",\"EFA\",\"GLD\",\"SLV\",\"TLT\",\"HYG\",\n",
    "    \"XLF\",\"XLK\",\"XLE\",\"XLY\",\"XLI\",\"XLV\",\"XLB\",\"XLRE\",\"XLC\",\"XLU\"\n",
    "]\n",
    "CRYPTO = [\n",
    "    \"BTC-USD\",\"ETH-USD\",\"SOL-USD\",\"BNB-USD\",\"ADA-USD\",\"XRP-USD\",\"DOGE-USD\",\n",
    "    \"AVAX-USD\",\"LTC-USD\",\"MATIC-USD\",\"DOT-USD\",\"TON-USD\"\n",
    "]\n",
    "\n",
    "UNIVERSE = sorted(list(set(STOCKS + ETFS + CRYPTO)))\n",
    "print(\"Universe size:\", len(UNIVERSE))\n",
    "print(\"Sample:\", UNIVERSE[:12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331f0cc",
   "metadata": {},
   "source": [
    "## 2) Data & Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e76551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_ohlcv(ticker: str, years: int = YEARS) -> pd.DataFrame | None:\n",
    "    end = datetime.utcnow()\n",
    "    start = end - timedelta(days=int(365.25 * years))\n",
    "    try:\n",
    "        df = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] {ticker} download failed: {e}\")\n",
    "        return None\n",
    "    if df is None or df.empty:\n",
    "        print(f\"[WARN] {ticker}: empty data\")\n",
    "        return None\n",
    "    df = df[[\"Close\",\"Volume\"]].dropna().copy()\n",
    "    df[\"Return1\"] = df[\"Close\"].pct_change()\n",
    "    return df\n",
    "\n",
    "def build_sequence_features(close_window: np.ndarray) -> np.ndarray:\n",
    "    # Ensure 1-D\n",
    "    x = np.asarray(close_window).astype(float).reshape(-1)\n",
    "    if len(x) < 2:\n",
    "        # Not enough data to compute returns\n",
    "        rets = np.zeros_like(x)\n",
    "    else:\n",
    "        rets = np.diff(x) / x[:-1]\n",
    "        rets = np.concatenate([[0.0], rets])  # align length\n",
    "\n",
    "    # Z-score\n",
    "    if x.std() == 0:\n",
    "        z = np.zeros_like(x)\n",
    "    else:\n",
    "        z = (x - x.mean()) / (x.std() + 1e-8)\n",
    "\n",
    "    # Scale raw closes\n",
    "    x_scaled = x / (x.mean() + 1e-8)\n",
    "\n",
    "    feats = np.stack([x_scaled, rets, z], axis=1)  # (L, 3)\n",
    "    return feats\n",
    "\n",
    "\n",
    "\n",
    "def build_samples(df: pd.DataFrame, look_back: int = LOOK_BACK, horizon: int = HORIZON):\n",
    "    closes = df[\"Close\"].values.astype(float)\n",
    "    Xs, ys = [], []\n",
    "    for i in range(look_back, len(df) - horizon):\n",
    "        win = closes[i - look_back:i]\n",
    "        Xs.append(build_sequence_features(win))\n",
    "        fut_ret = closes[i + horizon] / closes[i] - 1.0\n",
    "        ys.append(1 if fut_ret > 0 else 0)\n",
    "    if not Xs:\n",
    "        return None, None\n",
    "    X = np.stack(Xs)              # (N, L, F)\n",
    "    y = np.asarray(ys).astype(int)\n",
    "    return X, y\n",
    "\n",
    "def build_pooled(universe: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X_all, y_all = [], []\n",
    "    for t in universe:\n",
    "        df = fetch_ohlcv(t)\n",
    "        if df is None or len(df) < (LOOK_BACK + HORIZON + 50):\n",
    "            print(f\"[SKIP] {t}: not enough length\")\n",
    "            continue\n",
    "        X, y = build_samples(df)\n",
    "        if X is None:\n",
    "            print(f\"[SKIP] {t}: no samples built\")\n",
    "            continue\n",
    "        X_all.append(X); y_all.append(y)\n",
    "        print(f\"[OK] {t}: samples={len(y)}\")\n",
    "    if not X_all:\n",
    "        raise RuntimeError(\"No data built. Check tickers/network.\")\n",
    "    X = np.concatenate(X_all, axis=0)\n",
    "    y = np.concatenate(y_all, axis=0)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd8cb5b",
   "metadata": {},
   "source": [
    "## 3) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf02c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_model(n_features: int) -> keras.Model:\n",
    "    inp = layers.Input(shape=(LOOK_BACK, n_features))\n",
    "    x = layers.LSTM(96, return_sequences=False)(inp)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(48, activation=\"relu\")(x)\n",
    "    out = layers.Dense(1)(x)  # raw logit\n",
    "    model = keras.Model(inp, out)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
    "                  loss=keras.losses.BinaryCrossentropy(from_logits=True))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139725d3",
   "metadata": {},
   "source": [
    "## 4) Train with TimeSeriesSplit → Save Best → Refit & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51ffaba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] AAPL: samples=1185\n",
      "[OK] ABNB: samples=1111\n",
      "[OK] ADA-USD: samples=1757\n",
      "[OK] ADBE: samples=1185\n",
      "[OK] AMD: samples=1185\n",
      "[OK] AMZN: samples=1185\n",
      "[OK] ASML: samples=1185\n",
      "[OK] AVAX-USD: samples=1730\n",
      "[OK] BA: samples=1185\n",
      "[OK] BKNG: samples=1185\n",
      "[OK] BNB-USD: samples=1757\n",
      "[OK] BRK-B: samples=1185\n",
      "[OK] BTC-USD: samples=1757\n",
      "[OK] CAT: samples=1185\n",
      "[OK] COST: samples=1185\n",
      "[OK] CRM: samples=1185\n",
      "[OK] CSCO: samples=1185\n",
      "[OK] DIA: samples=1185\n",
      "[OK] DIS: samples=1185\n",
      "[OK] DOGE-USD: samples=1757\n",
      "[OK] DOT-USD: samples=1757\n",
      "[OK] EEM: samples=1185\n",
      "[OK] EFA: samples=1185\n",
      "[OK] ETH-USD: samples=1757\n",
      "[OK] GE: samples=1185\n",
      "[OK] GLD: samples=1185\n",
      "[OK] GOOGL: samples=1185\n",
      "[OK] HD: samples=1185\n",
      "[OK] HON: samples=1185\n",
      "[OK] HYG: samples=1185\n",
      "[OK] INTC: samples=1185\n",
      "[OK] IWM: samples=1185\n",
      "[OK] JNJ: samples=1185\n",
      "[OK] JPM: samples=1185\n",
      "[OK] KO: samples=1185\n",
      "[OK] LTC-USD: samples=1757\n",
      "[OK] MA: samples=1185\n",
      "[OK] MATIC-USD: samples=1602\n",
      "[OK] MCD: samples=1185\n",
      "[OK] META: samples=1185\n",
      "[OK] MRK: samples=1185\n",
      "[OK] MSFT: samples=1185\n",
      "[OK] NFLX: samples=1185\n",
      "[OK] NKE: samples=1185\n",
      "[OK] NVDA: samples=1185\n",
      "[OK] ORCL: samples=1185\n",
      "[OK] PEP: samples=1185\n",
      "[OK] PFE: samples=1185\n",
      "[OK] PG: samples=1185\n",
      "[OK] PYPL: samples=1185\n",
      "[OK] QQQ: samples=1185\n",
      "[OK] SAP: samples=1185\n",
      "[OK] SBUX: samples=1185\n",
      "[OK] SHOP: samples=1185\n",
      "[OK] SLV: samples=1185\n",
      "[OK] SNOW: samples=1171\n",
      "[OK] SOL-USD: samples=1757\n",
      "[OK] SPY: samples=1185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['SQ']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] SQ: empty data\n",
      "[SKIP] SQ: not enough length\n",
      "[OK] TLT: samples=1185\n",
      "[OK] TM: samples=1185\n",
      "[OK] TON-USD: samples=1754\n",
      "[OK] TSLA: samples=1185\n",
      "[OK] TSM: samples=1185\n",
      "[OK] UBER: samples=1185\n",
      "[OK] UNH: samples=1185\n",
      "[OK] UPS: samples=1185\n",
      "[OK] V: samples=1185\n",
      "[OK] WMT: samples=1185\n",
      "[OK] XLB: samples=1185\n",
      "[OK] XLC: samples=1185\n",
      "[OK] XLE: samples=1185\n",
      "[OK] XLF: samples=1185\n",
      "[OK] XLI: samples=1185\n",
      "[OK] XLK: samples=1185\n",
      "[OK] XLRE: samples=1185\n",
      "[OK] XLU: samples=1185\n",
      "[OK] XLV: samples=1185\n",
      "[OK] XLY: samples=1185\n",
      "[OK] XRP-USD: samples=1757\n",
      "Pooled: (100206, 60, 3) (100206,)\n",
      "[Fold 1] val_loss=0.6938\n",
      "[Fold 2] val_loss=0.6884\n",
      "[Fold 3] val_loss=0.6890\n",
      "[Fold 4] val_loss=0.6931\n",
      "[Fold 5] val_loss=0.6850\n",
      "Saved: /Users/nourf/Projects/stock-predictor/backend/notebooks/models/global/lstm_model.keras /Users/nourf/Projects/stock-predictor/backend/notebooks/models/global/scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build pooled dataset (this may take several minutes for ~100 tickers)\n",
    "X_pool, y_pool = build_pooled(UNIVERSE)\n",
    "N, L, F = X_pool.shape\n",
    "print(\"Pooled:\", X_pool.shape, y_pool.shape)\n",
    "\n",
    "# Flatten for scaling\n",
    "flat = X_pool.reshape(N*L, F)\n",
    "\n",
    "# TimeSeriesSplit cross-val to pick best model by val loss\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "best_w, best_val = None, 1e9\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(tscv.split(X_pool), start=1):\n",
    "    # Fit scaler on TRAIN indices only (approximate: use repeated indices to align shapes)\n",
    "    scaler = StandardScaler().fit(flat[tr_idx.repeat(L), :])\n",
    "    X_tr = scaler.transform(flat[tr_idx.repeat(L), :]).reshape(len(tr_idx), L, F)\n",
    "    X_va = scaler.transform(flat[va_idx.repeat(L), :]).reshape(len(va_idx), L, F)\n",
    "    y_tr, y_va = y_pool[tr_idx], y_pool[va_idx]\n",
    "\n",
    "    model = make_model(F)\n",
    "    cbs = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)]\n",
    "    hist = model.fit(X_tr, y_tr, validation_data=(X_va, y_va),\n",
    "                     epochs=EPOCHS, batch_size=BATCH, verbose=0, callbacks=cbs)\n",
    "    v = float(min(hist.history[\"val_loss\"]))\n",
    "    print(f\"[Fold {fold}] val_loss={v:.4f}\")\n",
    "    if v < best_val:\n",
    "        best_val, best_w = v, model.get_weights()\n",
    "\n",
    "# Final refit: scaler on first 90% (no leakage), early stop on last 10%\n",
    "cut = int(0.9 * N)\n",
    "scaler_all = StandardScaler().fit(flat[:cut*L, :])\n",
    "X_tr_all = scaler_all.transform(flat[:cut*L, :]).reshape(cut, L, F)\n",
    "X_va_all = scaler_all.transform(flat[cut*L:, :]).reshape(N - cut, L, F)\n",
    "y_tr_all = y_pool[:cut]\n",
    "y_va_all = y_pool[cut:]\n",
    "\n",
    "final = make_model(F)\n",
    "if best_w is not None:\n",
    "    final.set_weights(best_w)\n",
    "cbs = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)]\n",
    "final.fit(X_tr_all, y_tr_all, validation_data=(X_va_all, y_va_all),\n",
    "          epochs=EPOCHS, batch_size=BATCH, verbose=0, callbacks=cbs)\n",
    "\n",
    "# Save global model + scaler\n",
    "final.save(MODEL_PATH)\n",
    "joblib.dump(scaler_all, SCALER_PATH)\n",
    "print(\"Saved:\", MODEL_PATH, SCALER_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45dc4d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_calibrator(ticker: str, min_points: int = 50, months: int = CAL_WINDOW_MONTHS):\n",
    "    \"\"\"Return a dict {\"A\":..., \"B\":...}. If file missing, fit now and save.\"\"\"\n",
    "    path = os.path.join(CALIB_DIR, f\"{ticker.upper()}.json\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path) as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    # Fit on-demand\n",
    "    end = datetime.utcnow()\n",
    "    start = end - timedelta(days=int(30*months))\n",
    "    df = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n",
    "    if df is None or df.empty or len(df) < (LOOK_BACK + HORIZON + 30):\n",
    "        cal = {\"A\": 1.0, \"B\": 0.0, \"note\": \"fallback_identity_insufficient_data\"}\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(cal, f)\n",
    "        return cal\n",
    "\n",
    "    closes = df[\"Close\"].astype(float).values\n",
    "    scores, y_dir = [], []\n",
    "    for i in range(LOOK_BACK, len(closes)-HORIZON):\n",
    "        win = closes[i-LOOK_BACK:i]\n",
    "        X = build_sequence_features(win)\n",
    "        X = scaler_all.transform(X)[np.newaxis, ...]\n",
    "        logit = float(global_model.predict(X, verbose=0).ravel()[0])\n",
    "        fut = closes[i+HORIZON]/closes[i] - 1.0\n",
    "        scores.append(logit)\n",
    "        y_dir.append(1 if fut>0 else 0)\n",
    "\n",
    "    if len(scores) < min_points:\n",
    "        cal = {\"A\": 1.0, \"B\": 0.0, \"note\": \"fallback_identity_too_few_points\"}\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(cal, f)\n",
    "        return cal\n",
    "\n",
    "    lr = LogisticRegression(max_iter=500).fit(np.array(scores).reshape(-1,1), np.array(y_dir))\n",
    "    cal = {\"A\": float(lr.coef_.ravel()[0]), \"B\": float(lr.intercept_.ravel()[0])}\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(cal, f)\n",
    "    return cal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bdccc7",
   "metadata": {},
   "source": [
    "## 5) Fit Per‑Ticker Calibrators (last N months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bc156c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CAL]  AAPL: A=0.871 B=0.040\n",
      "[CAL]  ABNB: A=1.179 B=-0.106\n",
      "[CAL]  ADA-USD: A=0.942 B=-0.117\n",
      "[CAL]  ADBE: A=0.701 B=-0.424\n",
      "[CAL]  AMD: A=-0.306 B=1.212\n",
      "[CAL]  AMZN: A=-1.566 B=0.941\n",
      "[CAL]  ASML: A=0.227 B=0.397\n",
      "[CAL]  AVAX-USD: A=2.956 B=-0.267\n",
      "[CAL]  BA: A=-0.352 B=0.872\n",
      "[CAL]  BKNG: A=0.760 B=0.210\n",
      "[CAL]  BNB-USD: A=0.608 B=0.263\n",
      "[CAL]  BRK-B: A=0.573 B=-0.574\n",
      "[CAL]  BTC-USD: A=0.051 B=0.218\n",
      "[CAL]  CAT: A=-0.584 B=0.939\n",
      "[CAL]  COST: A=-0.190 B=0.303\n",
      "[CAL]  CRM: A=-0.364 B=-0.486\n",
      "[CAL]  CSCO: A=0.400 B=0.423\n",
      "[CAL]  DIA: A=0.635 B=0.325\n",
      "[CAL]  DIS: A=-0.280 B=0.368\n",
      "[CAL]  DOGE-USD: A=2.669 B=-0.087\n",
      "[CAL]  DOT-USD: A=2.360 B=-0.419\n",
      "[CAL]  EEM: A=-0.879 B=1.293\n",
      "[CAL]  EFA: A=-0.003 B=0.844\n",
      "[CAL]  ETH-USD: A=0.884 B=0.151\n",
      "[CAL]  GE: A=0.049 B=0.919\n",
      "[CAL]  GLD: A=0.096 B=0.502\n",
      "[CAL]  GOOGL: A=-1.427 B=1.270\n",
      "[CAL]  HD: A=-0.177 B=0.489\n",
      "[CAL]  HON: A=1.658 B=-0.189\n",
      "[CAL]  HYG: A=0.450 B=1.129\n",
      "[CAL]  INTC: A=-1.139 B=0.467\n",
      "[CAL]  IWM: A=-1.082 B=1.027\n",
      "[CAL]  JNJ: A=0.794 B=0.247\n",
      "[CAL]  JPM: A=0.024 B=0.715\n",
      "[CAL]  KO: A=0.180 B=-0.232\n",
      "[CAL]  LTC-USD: A=1.654 B=-0.077\n",
      "[CAL]  MA: A=0.133 B=0.423\n",
      "[SKIP] MATIC-USD: insufficient data for calibration\n",
      "[CAL]  MCD: A=0.270 B=0.131\n",
      "[CAL]  META: A=0.874 B=0.196\n",
      "[CAL]  MRK: A=1.268 B=-0.113\n",
      "[CAL]  MSFT: A=-1.249 B=1.562\n",
      "[CAL]  NFLX: A=0.305 B=0.513\n",
      "[CAL]  NKE: A=-1.651 B=0.520\n",
      "[CAL]  NVDA: A=-0.912 B=1.471\n",
      "[CAL]  ORCL: A=-0.801 B=0.979\n",
      "[CAL]  PEP: A=-1.094 B=0.260\n",
      "[CAL]  PFE: A=0.547 B=0.034\n",
      "[CAL]  PG: A=1.019 B=-0.707\n",
      "[CAL]  PYPL: A=-0.464 B=0.301\n",
      "[CAL]  QQQ: A=-1.215 B=1.416\n",
      "[CAL]  SAP: A=0.626 B=-0.161\n",
      "[CAL]  SBUX: A=0.843 B=-0.150\n",
      "[CAL]  SHOP: A=1.952 B=0.487\n",
      "[CAL]  SLV: A=-1.629 B=0.984\n",
      "[CAL]  SNOW: A=1.506 B=0.170\n",
      "[CAL]  SOL-USD: A=2.288 B=0.046\n",
      "[CAL]  SPY: A=-0.675 B=1.307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['SQ']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] SQ: insufficient data for calibration\n",
      "[CAL]  TLT: A=-0.305 B=0.085\n",
      "[CAL]  TM: A=0.864 B=-0.243\n",
      "[CAL]  TON-USD: A=1.375 B=-0.154\n",
      "[CAL]  TSLA: A=0.242 B=0.269\n",
      "[CAL]  TSM: A=-1.529 B=1.222\n",
      "[CAL]  UBER: A=-0.732 B=0.410\n",
      "[CAL]  UNH: A=0.295 B=-0.081\n",
      "[CAL]  UPS: A=0.452 B=-0.218\n",
      "[CAL]  V: A=0.970 B=0.103\n",
      "[CAL]  WMT: A=-0.225 B=0.497\n",
      "[CAL]  XLB: A=1.399 B=0.246\n",
      "[CAL]  XLC: A=1.250 B=0.664\n",
      "[CAL]  XLE: A=-1.179 B=0.530\n",
      "[CAL]  XLF: A=0.841 B=0.369\n",
      "[CAL]  XLI: A=1.130 B=0.495\n",
      "[CAL]  XLK: A=-1.667 B=1.491\n",
      "[CAL]  XLRE: A=-0.497 B=0.376\n",
      "[CAL]  XLU: A=0.745 B=0.735\n",
      "[CAL]  XLV: A=0.052 B=-0.315\n",
      "[CAL]  XLY: A=-1.253 B=0.922\n",
      "[CAL]  XRP-USD: A=1.055 B=-0.076\n",
      "Done. Calibrators OK=78, SKIP=2, ERR=0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reload saved artifacts for clarity\n",
    "global_model = keras.models.load_model(MODEL_PATH)\n",
    "scaler_all = joblib.load(SCALER_PATH)\n",
    "\n",
    "def fit_calibrator_for_ticker(ticker: str, months: int = CAL_WINDOW_MONTHS):\n",
    "    end = datetime.utcnow()\n",
    "    start = end - timedelta(days=int(30 * months))\n",
    "    df = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n",
    "    if df is None or df.empty or len(df) < (LOOK_BACK + HORIZON + 30):\n",
    "        return None\n",
    "    closes = df[\"Close\"].astype(float).values\n",
    "\n",
    "    scores, y_dir = [], []\n",
    "    for i in range(LOOK_BACK, len(closes) - HORIZON):\n",
    "        win = closes[i - LOOK_BACK:i]\n",
    "        X = build_sequence_features(win)             # (L, F)\n",
    "        X = scaler_all.transform(X)[np.newaxis, ...] # (1, L, F)\n",
    "        logit = float(global_model.predict(X, verbose=0).ravel()[0])\n",
    "        fut = closes[i + HORIZON] / closes[i] - 1.0\n",
    "        scores.append(logit)\n",
    "        y_dir.append(1 if fut > 0 else 0)\n",
    "\n",
    "    if len(scores) < 50:\n",
    "        return None\n",
    "\n",
    "    lr = LogisticRegression(max_iter=500).fit(np.array(scores).reshape(-1,1), np.array(y_dir))\n",
    "    A = float(lr.coef_.ravel()[0]); B = float(lr.intercept_.ravel()[0])\n",
    "    with open(os.path.join(CALIB_DIR, f\"{ticker.upper()}.json\"), \"w\") as f:\n",
    "        json.dump({\"A\":A, \"B\":B, \"updated_at\": datetime.utcnow().isoformat()}, f)\n",
    "    return {\"A\":A, \"B\":B}\n",
    "\n",
    "ok, skip, err = 0, 0, 0\n",
    "for t in UNIVERSE:\n",
    "    try:\n",
    "        cal = fit_calibrator_for_ticker(t, CAL_WINDOW_MONTHS)\n",
    "        if cal is None:\n",
    "            print(f\"[SKIP] {t}: insufficient data for calibration\")\n",
    "            skip += 1\n",
    "        else:\n",
    "            print(f\"[CAL]  {t}: A={cal['A']:.3f} B={cal['B']:.3f}\")\n",
    "            ok += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[ERR]  {t}: {e}\")\n",
    "        err += 1\n",
    "\n",
    "print(f\"Done. Calibrators OK={ok}, SKIP={skip}, ERR={err}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d5d07",
   "metadata": {},
   "source": [
    "## 6) Test Inference — Keep Frontend Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68261f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ticker\": \"AAPL\",\n",
      "  \"look_back\": 60,\n",
      "  \"context\": 60,\n",
      "  \"backtest_horizon\": 10,\n",
      "  \"horizon\": 10,\n",
      "  \"metrics\": {\n",
      "    \"direction_up_prob\": 0.5688731759178041\n",
      "  },\n",
      "  \"forecast\": [\n",
      "    {\n",
      "      \"date\": \"2025-07-29\",\n",
      "      \"actual\": 211.03050231933594\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"2025-07-30\",\n",
      "      \"actual\": 208.81301879882812\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"2025-07-31\",\n",
      "      \"actual\": 207.33470153808594\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"2025-08-01\",\n",
      "      \"actual\": 202.1505889892578\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"2025-08-04\",\n",
      "      \"actual\": 203.11949157714844\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"2025-08-05\",\n",
      "      \"actual\": 202.68995666503906\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"2025-08-06\",\n",
      "      \"actual\": 213.0082550048828\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"2025-08-07\",\n",
      "      \"actual\": 219.7805633544922\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"2025-08-08\",\n",
      "      \"actual\": 229.0900115966797\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"2025-08-11\",\n",
      "      \"actual\": 227.17999267578125\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"2025-08-12\",\n",
      "      \"actual\": 229.64999389648438\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"2025-08-13\",\n",
      "      \"actual\": 233.3300018310547\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"2025-08-14\",\n",
      "      \"actual\": 232.77999877929688\n",
      "    },\n",
      "    {\n",
      "      \"date\": \"2025-08-15\",\n",
      "  \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_calibrator(ticker: str) -> Dict[str,float]:\n",
    "    path = os.path.join(CALIB_DIR, f\"{ticker.upper()}.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return {\"A\":1.0, \"B\":0.0}\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def predict_direction_logit(ticker: str) -> tuple[float, float, pd.Timestamp]:\n",
    "    end = datetime.utcnow()\n",
    "    start = end - timedelta(days=int(LOOK_BACK * 3))\n",
    "    df = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n",
    "    if df is None or df.empty:\n",
    "        raise RuntimeError(f\"No data for {ticker}\")\n",
    "    closes = df[\"Close\"].astype(float).values\n",
    "    if len(closes) < LOOK_BACK + 1:\n",
    "        raise RuntimeError(\"Not enough data for sequence\")\n",
    "    win = closes[-LOOK_BACK:]\n",
    "    X = build_sequence_features(win)\n",
    "    X = scaler_all.transform(X)[np.newaxis, ...]\n",
    "    logit = float(global_model.predict(X, verbose=0).ravel()[0])\n",
    "    return logit, float(closes[-1]), df.index[-1]\n",
    "\n",
    "def make_forecast_series(ticker: str, last_close: float, last_date: pd.Timestamp, horizon: int = HORIZON):\n",
    "    fut_dates = []\n",
    "    d = last_date.to_pydatetime()\n",
    "    for _ in range(horizon):\n",
    "        d = d + timedelta(days=1)\n",
    "        while d.weekday() >= 5:\n",
    "            d = d + timedelta(days=1)\n",
    "        fut_dates.append(d)\n",
    "\n",
    "    preds = [float(last_close) for _ in range(horizon)]\n",
    "\n",
    "    start_hist = last_date - pd.Timedelta(days=40)\n",
    "    df = yf.download(ticker, start=start_hist, end=last_date + pd.Timedelta(days=1),\n",
    "                     auto_adjust=True, progress=False)\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        raise RuntimeError(f\"No historical data for {ticker}\")\n",
    "\n",
    "    # Always flatten to a Series with DatetimeIndex\n",
    "    hist = df[\"Close\"]\n",
    "    if isinstance(hist, pd.DataFrame):  # happens if yfinance gives multi-index\n",
    "        hist = hist.squeeze()           # collapse single column DataFrame\n",
    "    hist.index = pd.to_datetime(hist.index)\n",
    "\n",
    "    out = []\n",
    "    for d, v in hist[-20:].items():\n",
    "        if not isinstance(d, pd.Timestamp):   # if still str, convert\n",
    "            d = pd.to_datetime(d)\n",
    "        out.append({\n",
    "            \"date\": d.strftime(\"%Y-%m-%d\"),\n",
    "            \"actual\": float(v)\n",
    "        })\n",
    "\n",
    "    for d, p in zip(fut_dates, preds):\n",
    "        out.append({\n",
    "            \"date\": d.strftime(\"%Y-%m-%d\"),\n",
    "            \"predicted\": float(p)\n",
    "        })\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---- Try a sample ticker (change as needed)\n",
    "TEST_TICKER = \"AAPL\"\n",
    "\n",
    "logit, last_close, last_date = predict_direction_logit(TEST_TICKER)\n",
    "cal = ensure_calibrator(TEST_TICKER)\n",
    "p_up = float(sigmoid(cal[\"A\"]*logit + cal[\"B\"]))\n",
    "\n",
    "payload = {\n",
    "    \"ticker\": TEST_TICKER,\n",
    "    \"look_back\": LOOK_BACK,\n",
    "    \"context\": LOOK_BACK,\n",
    "    \"backtest_horizon\": HORIZON,\n",
    "    \"horizon\": HORIZON,\n",
    "    \"metrics\": {\"direction_up_prob\": p_up},\n",
    "    \"forecast\": make_forecast_series(TEST_TICKER, last_close, last_date, HORIZON),\n",
    "    \"recent_backtest\": backtest_last_n_days(TEST_TICKER, n_days=30)  # <-- optional, FE can ignore if not used\n",
    "}\n",
    "\n",
    "\n",
    "print(json.dumps(payload, indent=2)[:1200] + \"\\n...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4772823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last-30 leak-free backtest rows: 30\n",
      "[{'date': '2025-06-30', 'p_up': 0.5299735310963503, 'logit': 0.09152238070964813, 'actual_return_10d': 0.01920365298272375, 'actual_direction': 1}, {'date': '2025-07-01', 'p_up': 0.5357632319865273, 'logit': 0.11822635680437088, 'actual_return_10d': 0.011259788483777067, 'actual_direction': 1}, {'date': '2025-07-02', 'p_up': 0.5435035581974238, 'logit': 0.15399806201457977, 'actual_return_10d': -0.011391427935557585, 'actual_direction': 0}]\n"
     ]
    }
   ],
   "source": [
    "def backtest_last_n_days(ticker: str, n_days: int = 30):\n",
    "    \"\"\"\n",
    "    Leak-free rolling backtest for the most recent n_days *trading bars*.\n",
    "    It expands the download window until it has enough bars.\n",
    "    \"\"\"\n",
    "    # how many bars we need to make n_days forecasts\n",
    "    required = LOOK_BACK + HORIZON + n_days\n",
    "\n",
    "    # start with a generous calendar window; expand if needed\n",
    "    calendar_days = required + 60  # initial buffer\n",
    "    max_calendar_days = 365 * 2    # hard cap to avoid infinite loops\n",
    "\n",
    "    df = None\n",
    "    while True:\n",
    "        end = datetime.utcnow()\n",
    "        start = end - pd.Timedelta(days=calendar_days)\n",
    "        df = yf.download(ticker, start=start, end=end, interval=\"1d\",\n",
    "                         auto_adjust=True, progress=False)\n",
    "        if df is not None and not df.empty and len(df) >= required:\n",
    "            break\n",
    "        calendar_days = min(int(calendar_days * 1.5), max_calendar_days)\n",
    "        if calendar_days >= max_calendar_days:\n",
    "            raise RuntimeError(f\"Insufficient data for {ticker} even after expanding window \"\n",
    "                               f\"(got {0 if df is None else len(df)} bars, need {required}).\")\n",
    "\n",
    "    # use only the bars we actually downloaded\n",
    "    df = df.dropna()\n",
    "    closes = df[\"Close\"].astype(float).values\n",
    "    dates  = df.index.to_list()\n",
    "\n",
    "    # ensure we end on the last fully available bar\n",
    "    last_idx = len(closes) - 1\n",
    "\n",
    "    # ensure calibrator exists (fits on-demand if missing)\n",
    "    cal = ensure_calibrator(ticker)\n",
    "\n",
    "    rows = []\n",
    "    # we can predict for i in [LOOK_BACK .. last_idx - HORIZON]\n",
    "    # we want the most recent n_days of those predictions\n",
    "    start_i = max(LOOK_BACK, last_idx - HORIZON - n_days + 1)\n",
    "    end_i   = last_idx - HORIZON + 1\n",
    "\n",
    "    for i in range(start_i, end_i):\n",
    "        window = closes[i-LOOK_BACK:i]                # only data available up to i\n",
    "        X = build_sequence_features(window)\n",
    "        X = scaler_all.transform(X)[np.newaxis, ...]\n",
    "        logit = float(global_model.predict(X, verbose=0).ravel()[0])\n",
    "        p_up  = float(sigmoid(cal[\"A\"]*logit + cal[\"B\"]))\n",
    "\n",
    "        fut_ret = closes[i+HORIZON]/closes[i] - 1.0 if (i+HORIZON) <= last_idx else None\n",
    "        actual_dir = (1 if fut_ret > 0 else 0) if fut_ret is not None else None\n",
    "\n",
    "        d = dates[i]\n",
    "        if not isinstance(d, pd.Timestamp):\n",
    "            d = pd.to_datetime(d)\n",
    "        rows.append({\n",
    "            \"date\": d.strftime(\"%Y-%m-%d\"),\n",
    "            \"p_up\": p_up,\n",
    "            \"logit\": logit,\n",
    "            \"actual_return_10d\": float(fut_ret) if fut_ret is not None else None,\n",
    "            \"actual_direction\": actual_dir,\n",
    "        })\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "recent_bt = backtest_last_n_days(TEST_TICKER, n_days=30)\n",
    "print(\"Last-30 leak-free backtest rows:\", len(recent_bt))\n",
    "print(recent_bt[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9374cff0",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Notes & Tips\n",
    "\n",
    "- Keep the **`forecast` list shape** identical to your existing API for a drop-in replacement.\n",
    "- You can expand `metrics` with user-friendly KPIs (directional confidence, typical $ error).\n",
    "- Schedule this notebook nightly (or convert to a Python script) to refresh the model + calibrators.\n",
    "- If you need YOLO pattern integration, add a small API that takes a **chart image** and returns pattern detections (stateless) for the UI badges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
