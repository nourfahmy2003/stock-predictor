{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20941032",
   "metadata": {},
   "source": [
    "# Stock LSTM — Unified Backtest + 10‑day Forecast\n",
    "\n",
    "This notebook is parameterized for Papermill.\n",
    "\n",
    "**Inputs:** `TICKER, LOOKBACK, CONTEXT, BACKTEST_HORIZON, HORIZON, OUTPUT_JSON`\n",
    "\n",
    "**Output file format (JSON)**:\n",
    "```json\n",
    "{\n",
    "  \"ticker\": \"AAPL\",\n",
    "  \"look_back\": 60,\n",
    "  \"context\": 100,\n",
    "  \"backtest_horizon\": 20,\n",
    "  \"horizon\": 10,\n",
    "  \"metrics\": {\n",
    "    \"rmse\": 0.0,\n",
    "    \"mape\": 0.0,\n",
    "    \"accuracy_pct\": 0.0,\n",
    "    \"expected_10d_move_pct\": 0.0\n",
    "  },\n",
    "  \"forecast\": [\n",
    "    {\"date\": \"YYYY-MM-DD\", \"actual\": 123.45, \"part\": \"context\"},\n",
    "    {\"date\": \"YYYY-MM-DD\", \"pred\": 123.45, \"part\": \"backtest\"},\n",
    "    {\"date\": \"YYYY-MM-DD\", \"pred\": 123.45, \"part\": \"forecast\"}\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddf3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  imports\n",
    "import os, math, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import tensorflow as tf\n",
    "from joblib import dump, load\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c31424",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 2: parameters\n",
    "# Papermill parameters (will be overridden when executed by Papermill)\n",
    "TICKER = \"AAPL\"\n",
    "LOOKBACK = 60\n",
    "CONTEXT = 100\n",
    "BACKTEST_HORIZON = 20\n",
    "HORIZON = 10\n",
    "OUTPUT_JSON = \"forecast.json\"\n",
    "\n",
    "# Optional: batch list (one symbol per line). If you later want batch runs,\n",
    "# you can loop over EXAMPLE_TICKERS yourself in a separate cell.\n",
    "TICKERS_FILE = \"tickers_1000_plus.txt\"\n",
    "INTERVAL = \"1d\"\n",
    "\n",
    "# Control retraining / calibration behavior\n",
    "FORCE_RETRAIN = False         # set True to ignore saved models & retrain\n",
    "ENABLE_CALIBRATOR = True      # per-ticker linear post-adjustment\n",
    "\n",
    "# Load example/batch tickers if file exists\n",
    "if Path(TICKERS_FILE).exists():\n",
    "    EXAMPLE_TICKERS = [l.strip() for l in Path(TICKERS_FILE).read_text().splitlines() if l.strip()]\n",
    "else:\n",
    "    EXAMPLE_TICKERS = [\n",
    "        \"SPY\",\"QQQ\",\"IWM\",\"EFA\",\"EEM\",\"GLD\",\"SLV\",\"TLT\",\"HYG\",\"XLF\",\"XLE\",\"XLY\",\"XLK\",\"XLV\",\"XLI\",\"XLP\",\"XLB\",\"XLU\",\"VNQ\",\"ARKK\",\n",
    "        \"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\",\"META\",\"TSLA\",\"BRK-B\",\"JPM\",\"V\",\"JNJ\",\"WMT\",\"PG\",\"UNH\",\"MA\",\"HD\",\"XOM\",\"BAC\",\"PFE\",\"DIS\",\n",
    "        \"BTC-USD\",\"ETH-USD\",\"SOL-USD\",\"BNB-USD\",\"XRP-USD\",\"ADA-USD\",\"DOGE-USD\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9eb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: helper functions & model\n",
    "ARTIFACTS_DIR = Path(\"artifacts\")\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def fetch_prices(ticker: str, start=\"2016-01-01\", end=None, interval=\"1d\") -> pd.DataFrame:\n",
    "    df = yf.download(ticker, start=start, end=end, interval=interval, auto_adjust=True, progress=False, timeout=30)\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"No data returned.\")\n",
    "    df.index = pd.to_datetime(df.index); df.index.name = \"Date\"\n",
    "    cols = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"] if c in df.columns]\n",
    "    if \"Close\" not in cols and \"Adj Close\" in df.columns:\n",
    "        df[\"Close\"] = df[\"Adj Close\"]; cols = [*cols, \"Close\"]\n",
    "    return df[cols].dropna()\n",
    "\n",
    "def fetch_prices_auto(ticker: str, interval=\"1d\") -> pd.DataFrame:\n",
    "    for p in [\"1y\",\"150d\",\"90d\",\"60d\",\"30d\"]:\n",
    "        try:\n",
    "            df = yf.download(ticker, period=p, interval=interval, auto_adjust=True, progress=False, timeout=30)\n",
    "            if df is not None and not df.empty:\n",
    "                df = df[[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]].dropna()\n",
    "                if not df.empty:\n",
    "                    df.index = pd.to_datetime(df.index); df.index.name = \"Date\"\n",
    "                    return df\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise ValueError(\"No data for any fallback period.\")\n",
    "\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    close = pd.to_numeric(out[\"Close\"].squeeze(), errors=\"coerce\")\n",
    "    close = close.where(close > 0, np.nan)\n",
    "    out[\"Close\"] = close\n",
    "    out[\"log_ret\"] = np.log(close).diff()\n",
    "    out[\"ret\"] = close.pct_change()\n",
    "    out[\"roll_mean_7\"] = close.rolling(7).mean()\n",
    "    out[\"roll_std_7\"] = close.rolling(7).std()\n",
    "    out[\"roll_mean_21\"] = close.rolling(21).mean()\n",
    "    out[\"roll_std_21\"] = close.rolling(21).std()\n",
    "    delta = close.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -delta.clip(upper=0)\n",
    "    roll_up = up.ewm(alpha=1 / 14, min_periods=14, adjust=False).mean()\n",
    "    roll_dn = down.ewm(alpha=1 / 14, min_periods=14, adjust=False).mean()\n",
    "    rs = roll_up / roll_dn.replace(0, np.nan)\n",
    "    out[\"rsi_14\"] = 100 - (100 / (1 + rs))\n",
    "    ema12 = close.ewm(span=12, adjust=False).mean()\n",
    "    ema26 = close.ewm(span=26, adjust=False).mean()\n",
    "    macd = ema12 - ema26\n",
    "    out[\"macd\"] = macd\n",
    "    out[\"macd_signal\"] = macd.ewm(span=9, adjust=False).mean()\n",
    "    out[\"macd_diff\"] = out[\"macd\"] - out[\"macd_signal\"]\n",
    "    ma20 = close.rolling(20).mean()\n",
    "    sd20 = close.rolling(20).std()\n",
    "    out[\"bb_width\"] = (ma20 + 2 * sd20 - (ma20 - 2 * sd20)) / close\n",
    "    out[\"ret_lag1\"] = out[\"log_ret\"].shift(1)\n",
    "    out[\"ret_lag3\"] = out[\"log_ret\"].shift(3)\n",
    "    out[\"ret_lag5\"] = out[\"log_ret\"].shift(5)\n",
    "    out[\"vol_7\"] = out[\"log_ret\"].rolling(7).std()\n",
    "    out[\"vol_21\"] = out[\"log_ret\"].rolling(21).std()\n",
    "    out[\"z_close_21\"] = (close - close.rolling(21).mean()) / close.rolling(21).std()\n",
    "    return out.dropna()\n",
    "\n",
    "FEATURES = [\n",
    "    \"Close\",\"Volume\",\"log_ret\",\"ret\",\n",
    "    \"roll_mean_7\",\"roll_std_7\",\"roll_mean_21\",\"roll_std_21\",\n",
    "    \"rsi_14\",\"macd\",\"macd_signal\",\"macd_diff\",\n",
    "    \"bb_width\",\"ret_lag1\",\"ret_lag3\",\"ret_lag5\",\n",
    "    \"vol_7\",\"vol_21\",\"z_close_21\",\n",
    "]\n",
    "\n",
    "def make_windows(X: np.ndarray, y: np.ndarray, lookback: int, horizon: int):\n",
    "    xs, ys = [], []\n",
    "    for i in range(lookback, len(X) - horizon + 1):\n",
    "        xs.append(X[i - lookback : i, :])\n",
    "        ys.append(y[i : i + horizon])\n",
    "    return np.array(xs, dtype=\"float32\"), np.array(ys, dtype=\"float32\")\n",
    "\n",
    "def build_model(input_steps: int, n_features: int, horizon: int) -> tf.keras.Model:\n",
    "    inp = tf.keras.Input(shape=(input_steps, n_features))\n",
    "    x = tf.keras.layers.Conv1D(48, kernel_size=5, padding=\"causal\", activation=\"relu\")(inp)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(160, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.LSTM(96)(x)\n",
    "    out = tf.keras.layers.Dense(horizon)(x)\n",
    "    model = tf.keras.Model(inp, out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def infer_freq_has_weekends(idx: pd.Index) -> bool:\n",
    "    try:\n",
    "        weekdays = pd.Index([pd.Timestamp(x).weekday() for x in idx])\n",
    "        return (weekdays >= 5).any()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def next_dates(last_date, n: int, use_weekends: bool) -> list:\n",
    "    last_ts = pd.Timestamp(last_date)\n",
    "    if use_weekends:\n",
    "        rng = pd.date_range(last_ts, periods=n+1, freq=\"D\")\n",
    "        return [d for d in rng[1:]]\n",
    "    else:\n",
    "        rng = pd.bdate_range(last_ts, periods=n+1)\n",
    "        return [d for d in rng[1:]]\n",
    "\n",
    "def _sig(ticker:str, lookback:int, horizon:int):\n",
    "    t = ticker.replace('/','-')\n",
    "    return f\"{t}_L{lookback}_H{horizon}\"\n",
    "\n",
    "def _paths(ticker:str, lookback:int, horizon:int):\n",
    "    s = _sig(ticker, lookback, horizon)\n",
    "    mp_multi = ARTIFACTS_DIR / f\"lstm_multi_{s}.keras\"\n",
    "    mp_one   = ARTIFACTS_DIR / f\"lstm_one_{s}.keras\"\n",
    "    sx_path  = ARTIFACTS_DIR / f\"scalerX_{s}.joblib\"\n",
    "    sy_path  = ARTIFACTS_DIR / f\"scalerY_{s}.joblib\"\n",
    "    cal_path = ARTIFACTS_DIR / \"calibrators.json\"\n",
    "    return mp_multi, mp_one, sx_path, sy_path, cal_path\n",
    "\n",
    "def _load_calibrators(path:Path):\n",
    "    if path.exists():\n",
    "        try: return json.loads(path.read_text())\n",
    "        except Exception: return {}\n",
    "    return {}\n",
    "\n",
    "def _save_calibrators(path:Path, obj:dict):\n",
    "    path.write_text(json.dumps(obj, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfee9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: main logic (saves/loads models, builds calibrator, writes JSON)\n",
    "# 1) Pull enough data (smart fallback if not enough)\n",
    "buffer_days = 320\n",
    "start_date = (pd.Timestamp.utcnow() - pd.Timedelta(days=CONTEXT + LOOKBACK + BACKTEST_HORIZON + buffer_days)).date().isoformat()\n",
    "try:\n",
    "    raw = fetch_prices(TICKER, start=start_date, interval=INTERVAL)\n",
    "    df = add_features(raw)\n",
    "    if len(df) < (LOOKBACK + BACKTEST_HORIZON + CONTEXT + 10):\n",
    "        raise ValueError(\"fallback\")\n",
    "except Exception:\n",
    "    raw = fetch_prices_auto(TICKER, interval=INTERVAL)\n",
    "    df = add_features(raw)\n",
    "\n",
    "df[\"target_ret\"] = df[\"log_ret\"].shift(-1)\n",
    "df = df.dropna()\n",
    "if len(df) < LOOKBACK + BACKTEST_HORIZON + 5:\n",
    "    raise ValueError(\"Not enough rows after feature engineering.\")\n",
    "\n",
    "# 2) Keep just the window we need for context + train + backtest\n",
    "df_tail = df.tail(CONTEXT + LOOKBACK + BACKTEST_HORIZON)\n",
    "X_df = df_tail[FEATURES].astype(\"float32\")\n",
    "y = df_tail[\"target_ret\"].astype(\"float32\").values\n",
    "dates = df_tail.index\n",
    "\n",
    "train_end_idx = len(df_tail) - BACKTEST_HORIZON\n",
    "if train_end_idx <= LOOKBACK:\n",
    "    train_end_idx = LOOKBACK + 1\n",
    "    BACKTEST_HORIZON = max(1, len(df_tail) - train_end_idx)\n",
    "\n",
    "X_train_df = X_df.iloc[:train_end_idx]\n",
    "y_train = y[:train_end_idx]\n",
    "n_features = X_train_df.shape[1]\n",
    "\n",
    "# 3) Scalers on train only (load if present)\n",
    "mp_multi, mp_one, sx_path, sy_path, cal_path = _paths(TICKER, LOOKBACK, HORIZON)\n",
    "if (not FORCE_RETRAIN) and sx_path.exists() and sy_path.exists():\n",
    "    scaler_X = load(sx_path)\n",
    "    scaler_y = load(sy_path)\n",
    "else:\n",
    "    scaler_X = StandardScaler().fit(X_train_df.values)\n",
    "    scaler_y = StandardScaler().fit(y_train.reshape(-1,1))\n",
    "    dump(scaler_X, sx_path)\n",
    "    dump(scaler_y, sy_path)\n",
    "\n",
    "X_train = scaler_X.transform(X_train_df.values)\n",
    "y_train_s = scaler_y.transform(y_train.reshape(-1,1)).ravel()\n",
    "\n",
    "# 4) Train or load models\n",
    "Xw,  yw  = make_windows(X_train, y_train_s, LOOKBACK, HORIZON)\n",
    "Xw1, yw1 = make_windows(X_train, y_train_s, LOOKBACK, 1)\n",
    "\n",
    "if (not FORCE_RETRAIN) and mp_multi.exists():\n",
    "    m_multi = tf.keras.models.load_model(mp_multi)\n",
    "else:\n",
    "    if len(Xw) == 0: raise ValueError(\"No training windows for multi-step model.\")\n",
    "    m_multi = build_model(LOOKBACK, n_features, HORIZON)\n",
    "    cbs=[tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=6, restore_best_weights=True)]\n",
    "    m_multi.fit(Xw, yw, epochs=24, batch_size=32, verbose=0, callbacks=cbs)\n",
    "    m_multi.save(mp_multi)\n",
    "\n",
    "if (not FORCE_RETRAIN) and mp_one.exists():\n",
    "    m_one = tf.keras.models.load_model(mp_one)\n",
    "else:\n",
    "    if len(Xw1) == 0: raise ValueError(\"No training windows for 1-step model.\")\n",
    "    m_one = build_model(LOOKBACK, n_features, 1)\n",
    "    cbs=[tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=6, restore_best_weights=True)]\n",
    "    m_one.fit(Xw1, yw1, epochs=16, batch_size=32, verbose=0, callbacks=cbs)\n",
    "    m_one.save(mp_one)\n",
    "\n",
    "# 5) Backtest last BACKTEST_HORIZON days (1-step each)\n",
    "backtest_dates, backtest_pred_prices, backtest_actual_prices, backtest_pred_rets = [], [], [], []\n",
    "for k in range(int(BACKTEST_HORIZON)):\n",
    "    end_idx = train_end_idx + k\n",
    "    xb_raw = X_df.values[end_idx-LOOKBACK:end_idx, :]\n",
    "    xb = scaler_X.transform(xb_raw).reshape(1, LOOKBACK, n_features)\n",
    "    pred_ret_s = m_one.predict(xb, verbose=0)[0][0]\n",
    "    pred_ret = scaler_y.inverse_transform([[pred_ret_s]])[0,0]\n",
    "    last_price = float(df_tail[\"Close\"].iloc[end_idx-1])\n",
    "    pred_price = float(last_price * np.exp(pred_ret))\n",
    "    actual_price = float(df_tail[\"Close\"].iloc[end_idx])\n",
    "    backtest_dates.append(pd.Timestamp(dates[end_idx]).strftime(\"%Y-%m-%d\"))\n",
    "    backtest_pred_prices.append(pred_price)\n",
    "    backtest_actual_prices.append(actual_price)\n",
    "    backtest_pred_rets.append(float(pred_ret))\n",
    "\n",
    "rmse = float(np.sqrt(mean_squared_error(backtest_actual_prices, backtest_pred_prices)))\n",
    "mape = float(mean_absolute_percentage_error(backtest_actual_prices, backtest_pred_prices) * 100)\n",
    "\n",
    "actual_rets = []\n",
    "for k in range(int(BACKTEST_HORIZON)):\n",
    "    prev_price = float(df_tail[\"Close\"].iloc[train_end_idx - 1 + k])\n",
    "    cur_price = float(df_tail[\"Close\"].iloc[train_end_idx + k])\n",
    "    actual_rets.append((cur_price - prev_price) / (prev_price if prev_price != 0 else 1.0))\n",
    "\n",
    "acc = float((np.sign(backtest_pred_rets) == np.sign(actual_rets)).mean() * 100.0) if backtest_pred_rets and actual_rets else 0.0\n",
    "\n",
    "# 6) Forward multi-step forecast with optional per-ticker calibrator\n",
    "last_block_raw = X_df.values[-LOOKBACK:]\n",
    "last_block = scaler_X.transform(last_block_raw).reshape(1, LOOKBACK, n_features)\n",
    "next_rets_s = m_multi.predict(last_block, verbose=0)[0]\n",
    "next_rets = scaler_y.inverse_transform(next_rets_s.reshape(-1,1)).ravel()\n",
    "\n",
    "if ENABLE_CALIBRATOR:\n",
    "    key = _sig(TICKER, LOOKBACK, HORIZON)\n",
    "    cals = _load_calibrators(cal_path)\n",
    "    if len(backtest_pred_rets) >= 5:\n",
    "        A = np.vstack([backtest_pred_rets, np.ones(len(backtest_pred_rets))]).T\n",
    "        sol, *_ = np.linalg.lstsq(A, np.array(actual_rets[:len(backtest_pred_rets)]), rcond=None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
